{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2k4wwjxHLjH",
        "outputId": "3c2fa150-2a56-45fa-b7ba-f591156bdf22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 21 06:50:50 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x5nFSN2HH5F"
      },
      "source": [
        "# Get content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz1f7LPiHads",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13cf67d3-5fe1-441e-8733-3a7a26845a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu -q\n",
        "!pip install beautifulsoup4 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "7B7GagRZHH5H"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "query = \"RAG system?\"\n",
        "\n",
        "\n",
        "url = f\"https://www.google.com/search?q={query.replace(' ', '+')}\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    urls = []\n",
        "    gfg = 0\n",
        "    for g in soup.find_all('a'):\n",
        "        href = g.get('href')\n",
        "        if href and \"/url?esrc=s&q=&rct=j&sa=U&url=\" in href:\n",
        "            link = href.split(\"/url?esrc=s&q=&rct=j&sa=U&url=\")[1].split(\"&\")[0]\n",
        "            #print(link)\n",
        "            #if (link.split(\"https://\")[1].split(\".\")[0] != \"scholar\"):\n",
        "            urls.append(link)\n",
        "else:\n",
        "    print(\"Failed to fetch results\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsEyhNksCOOO",
        "outputId": "4ba30eb4-894d-4cfd-c592-ca8eeed2b183"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUu0I5HcVET3",
        "outputId": "5b22679d-190f-4ef9-8532-d29f35fe265c"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://cloud.google.com/use-cases/retrieval-augmented-generation',\n",
              " 'https://aws.amazon.com/what-is/retrieval-augmented-generation/',\n",
              " 'https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/',\n",
              " 'https://www.oracle.com/ie/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/',\n",
              " 'https://www.oracle.com/in/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/',\n",
              " 'https://www.oracle.com/sg/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/',\n",
              " 'https://www.oracle.com/uk/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/',\n",
              " 'https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/',\n",
              " 'https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag',\n",
              " 'https://en.wikipedia.org/wiki/Retrieval-augmented_generation',\n",
              " 'https://www.willowtreeapps.com/craft/retrieval-augmented-generation',\n",
              " 'https://research.ibm.com/blog/retrieval-augmented-generation-RAG',\n",
              " 'https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview',\n",
              " 'https://learn.microsoft.com/en-us/azure/search/tutorial-rag-build-solution',\n",
              " 'https://learn.microsoft.com/en-us/azure/search/search-get-started-rag',\n",
              " 'https://learn.microsoft.com/en-us/azure/ai-studio/concepts/retrieval-augmented-generation',\n",
              " 'https://learn.microsoft.com/en-us/azure/search/search-query-overview']"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "final_urls = []\n",
        "total = 0\n",
        "for url in urls:\n",
        "    success = False\n",
        "    while not success:\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                for tag in soup(['script', 'style', 'header', 'footer', 'nav']):\n",
        "                    tag.decompose()\n",
        "\n",
        "                page_text = ' '.join(tag.get_text() for tag in soup.find_all(['p', 'h1', 'h2', 'h3', 'li']))\n",
        "                texts.append(page_text)\n",
        "                success = True\n",
        "                total += 1\n",
        "                final_urls.append(url)\n",
        "            else:\n",
        "                print(f\"Failed to fetch {url}: {response.status_code}\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "            break\n",
        "\n",
        "    if (total == 3):\n",
        "        break\n"
      ],
      "metadata": {
        "id": "o1--aV5iIzcz"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_urls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkzNLQf8xc8J",
        "outputId": "63aaa44e-9391-43fd-9661-1102ac66bda1"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://cloud.google.com/use-cases/retrieval-augmented-generation',\n",
              " 'https://aws.amazon.com/what-is/retrieval-augmented-generation/',\n",
              " 'https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/']"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1l6QKFZRweK",
        "outputId": "446518f9-90a1-42fa-fbad-8c95fc406f8e"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What is Retrieval-Augmented Generation (RAG)? RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs). By combining your data and world knowledge with LLM language skills, grounded generation is more accurate, up-to-date, and relevant to your specific needs. Check out this e-book to unlock your “Enterprise Truth.” How does Retrieval-Augmented Generation work? RAGs operate with a few main steps to help enhance generative AI outputs:\\xa0 Retrieval and pre-processing: RAGs leverage powerful search algorithms to query external data, such as web pages, knowledge bases, and databases. Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words. Grounded generation: The pre-processed retrieved information is then seamlessly incorporated into the pre-trained LLM. This integration enhances the LLM\\'s context, providing it with a more comprehensive understanding of the topic. This augmented context enables the LLM to generate more precise, informative, and engaging responses.\\xa0 Why Use RAG? RAG offers several advantages augmenting traditional methods of text generation, especially when dealing with factual information or data-driven responses. Here are some key reasons why using RAG can be beneficial: Access to fresh information LLMs are limited to their pre-trained data. This leads to outdated and potentially inaccurate responses. RAG overcomes this by providing up-to-date information to\\xa0LLMs. Factual grounding LLMs are powerful tools for generating creative and engaging text, but they can sometimes struggle with factual accuracy. This is because LLMs are trained on massive amounts of text data, which may contain inaccuracies or biases. Providing “facts” to the LLM as part of the input prompt can mitigate “gen AI hallucinations.”\\xa0The crux of this approach is ensuring that the most relevant facts are provided to the LLM, and that the LLM output is entirely grounded on those facts while also answering the user’s question and adhering to system instructions and safety constraints. Using Gemini’s long context window (LCW) is a great way to provide source materials to the LLM.\\xa0If you need to provide more information than fits into the LCW, or if you need to scale up performance, you can use a RAG approach that will reduce the number of tokens, saving you time and cost. Search with vector databases and relevancy re-rankers RAGs usually retrieve facts via search, and modern search engines now leverage vector databases to efficiently retrieve relevant documents. Vector databases store documents as embeddings in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity.\\xa0Multi-modal embeddings can be used for images, audio and video, and more and these media embeddings can be retrieved alongside text embeddings or multi-language embeddings. Advanced search engines like Vertex AI Search use semantic search and keyword search together (called hybrid search), and a re-ranker which scores search results to ensure the top returned results are the most relevant.\\xa0Additionally searches perform better with a clear, focused query without misspellings; so prior to lookup, sophisticated search engines will transform a query and fix spelling mistakes. Relevance, accuracy, and quality The retrieval mechanism in RAG is critically important.\\xa0You need the best semantic search on top of a curated knowledge base to ensure that the retrieved information is relevant to the input query or context.\\xa0If your retrieved information is irrelevant, your generation could be grounded but off-topic or incorrect. By fine-tuning or prompt-engineering the LLM to generate text entirely based on the retrieved knowledge, RAG helps to minimize contradictions and inconsistencies in the generated text.\\xa0This significantly improves the quality of the generated text, and improves the user experience. The Vertex Eval Service now scores LLM generated text and retrieved chunks on metrics like “coherence,” “fluency,” “groundedness,” \"safety,\" “instruction_following,” “question_answering_quality,” and more.\\xa0These metrics help you measure the grounded text you get from the LLM (for some metrics that is a comparison to a ground truth answer you have provided).\\xa0Implementing these evaluations gives you a baseline measurement and you can optimize for RAG quality by configuring your search engine, curating your source data, improving source layout parsing or chunking strategies, or refining the user’s question prior to search.\\xa0A RAG Ops, metrics driven approach like this will help you hill climb to high quality RAG and grounded generation. RAGs, agents, and chatbots RAG and grounding can be integrated into any LLM application or agent which needs access to fresh, private, or specialized data. By accessing external information, RAG-powered chatbots and conversational agents leverage external knowledge to provide more comprehensive, informative, and context-aware responses, improving the overall user experience. Your data and your use case are what differentiate what you are building with gen AI.\\xa0RAG and grounding bring your data to LLMs efficiently and scalably. What Google Cloud products and services are related to RAG? The following Google Cloud products are related to Retrieval-Augmented Generation: Vertex AI SearchVertex AI Search is Google Search for your data, a fully managed, out-of-the-box search and RAG builder. Vertex AI Vector SearchThe ultra performant vector index that powers Vertex AI Search; it enables semantic and hybrid search and retrieval from huge collections of embeddings with high recall at high query rate. BigQueryLarge datasets that you can use to train machine learning models, including models for Vertex AI Vector Search. Grounded Generation APIGemini high-fidelity mode grounded with Google Search or inline facts or bring your own search engine. AlloyDBRun models in Vertex AI and access them in your application using familiar SQL queries. Use Google models, such as Gemini, or your own custom models. LlamaIndex on VertexBuild your own search engine for RAG and grounding using Google or open source components and our fully managed orchestration system based on LlamaIndex. Learn more about using retrieval augmented generation with these resources. Using Vertex AI to build next-gen search applications | Google Cloud Blog RAGs powered by Google Search technology RAG with databases on Google Cloud Infrastructure for a RAG-capable generative AI application using Vertex AI APIs to build your own search and Retrieval Augmented Generation (RAG) systems How to use RAG in BigQuery to bolster LLMs Code sample and quickstart to get familiar with RAG Start building on Google Cloud with $300 in free credits and 20+ always free products. Need help getting started?Contact sales Work with a trusted partnerFind a partner Continue browsingSee all products',\n",
              " 'What is Cloud Computing? Cloud Computing Concepts Hub Artificial Intelligence Generative AI What is RAG (Retrieval-Augmented Generation)? What is Retrieval-Augmented Generation? Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization\\'s internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. Why is Retrieval-Augmented Generation important? LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has. Known challenges of LLMs include: Presenting false information when it does not have the answer. Presenting out-of-date or generic information when the user expects a specific, current response. Creating a response from non-authoritative sources. Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things. You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate! RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response. What are the benefits of Retrieval-Augmented Generation? RAG technology brings several benefits to an organization\\'s generative AI efforts. Cost-effective implementation Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable. Current information Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users. Enhanced user trust RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution. More developer control With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM\\'s information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications. How does Retrieval-Augmented Generation work? Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process. Create external data The new data outside of the LLM\\'s original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand. Retrieve relevant information The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee\\'s past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations. Augment the LLM prompt Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries. Update external data The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used. The following diagram shows the conceptual flow of using RAG with LLMs.  \\xa0 What is the difference between Retrieval-Augmented Generation and semantic search? Semantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications. Modern enterprises store vast amounts of information like manuals, FAQs, research reports, customer service guides, and human resource document repositories across various systems. Context retrieval is challenging at scale and consequently lowers generative output quality. Semantic search technologies can scan large databases of disparate information and retrieve data more accurately. For example, they can answer questions such as, \"How much was spent on machinery repairs last year?” by mapping the question to the relevant documents and returning specific text instead of search results. Developers can then use that answer to provide more context to the LLM. Conventional or keyword search solutions in RAG produce limited results for knowledge-intensive tasks. Developers must also deal with word embeddings, document chunking, and other complexities as they manually prepare their data. In contrast, semantic search technologies do all the work of knowledge base preparation so developers don\\'t have to. They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload. How can AWS support your\\xa0Retrieval-Augmented Generation requirements? Amazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models—along with a broad set of capabilities—to build generative AI applications while simplifying development and maintaining privacy and security. With knowledge bases for Amazon Bedrock, you can connect FMs to your data sources for RAG in just a few clicks. Vector conversions, retrievals, and improved output generation are all handled automatically. For organizations managing their own RAG, Amazon Kendra is a highly-accurate enterprise search service powered by machine learning. It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra’s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows. For example, with the Retrieve API, you can: Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance. Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites. Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files. Filter responses based on those documents that the end-user permissions allow. Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples. Get started with Retrieval-Augmented Generation on AWS by creating a free account today  Next Steps on AWS Instant get access to the AWS Free Tier. Get started building in the AWS management console.',\n",
              " 'Accessibility Policy Skip to content QUICK LINKS Oracle Cloud Infrastructure Oracle Fusion Cloud Applications Oracle Database Download Java Careers at Oracle What Is Retrieval-Augmented Generation (RAG)? Alan Zeichick | Tech Content Strategist | September 19, 2023 In This Article What Is Retrieval-Augmented Generation (RAG)? Retrieval-Augmented Generation Explained How Does Retrieval-Augmented Generation Work? Using RAG in Chat Applications Benefits of Retrieval-Augmented Generation Challenges of Retrieval-Augmented Generation Examples of Retrieval-Augmented Generation Future of Retrieval-Augmented Generation Generative AI With Oracle Retrieval-Augmented Generation FAQs Generative artificial intelligence (AI) excels at creating text responses based on large language models (LLMs) where the AI is trained on a massive number of data points. The good news is that the generated text is often easy to read and provides detailed responses that are broadly applicable to the questions asked of the software, often called prompts. The bad news is that the information used to generate the response is limited to the information used to train the AI, often a generalized LLM. The LLM’s data may be weeks, months, or years out of date and in a corporate AI chatbot may not include specific information about the organization’s products or services. That can lead to incorrect responses that erode confidence in the technology among customers and employees. What Is Retrieval-Augmented Generation (RAG)? That’s where retrieval-augmented generation (RAG) comes in. RAG provides a way to optimize the output of an LLM with targeted information without modifying the underlying model itself; that targeted information can be more up-to-date than the LLM as well as specific to a particular organization and industry. That means the generative AI system can provide more contextually appropriate answers to prompts as well as base those answers on extremely current data. RAG first came to the attention of generative AI developers after the publication of “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” a 2020 paper published by Patrick Lewis and a team at Facebook AI Research. The RAG concept has been embraced by many academic and industry researchers, who see it as a way to significantly improve the value of generative AI systems. Retrieval-Augmented Generation Explained Consider a sports league that wants fans and the media to be able to use chat to access its data and answer questions about players, teams, the sport’s history and rules, and current stats and standings. A generalized LLM could answer questions about the history and rules or perhaps describe a particular team’s stadium. It wouldn’t be able to discuss last night’s game or provide current information about a particular athlete’s injury because the LLM wouldn’t have that information—and given that an LLM takes significant computing horsepower to retrain, it isn’t feasible to keep the model current. In addition to the large, fairly static LLM, the sports league owns or can access many other information sources, including databases, data warehouses, documents containing player bios, and news feeds that discuss each game in depth. RAG lets the generative AI ingest this information. Now, the chat can provide information that’s more timely, more contextually appropriate, and more accurate.  Simply put, RAG helps LLMs give better answers. Key Takeaways RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining. RAG models build knowledge repositories based on the organization’s own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers. Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI. Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM. How Does Retrieval-Augmented Generation Work?  Consider all the information that an organization has—the structured databases, the unstructured PDFs and other documents, the blogs, the news feeds, the chat transcripts from past customer service sessions. In RAG, this vast quantity of dynamic data is translated into a common format and stored in a knowledge library that’s accessible to the generative AI system.  The data in that knowledge library is then processed into numerical representations using a special type of algorithm called an embedded language model and stored in a vector database, which can be quickly searched and used to retrieve the correct contextual information. RAG and Large Language Models (LLMs) Now, say an end user sends the generative AI system a specific prompt, for example, “Where will tonight’s game be played, who are the starting players, and what are reporters saying about the matchup?” The query is transformed into a vector and used to query the vector database, which retrieves information relevant to that question’s context. That contextual information plus the original prompt are then fed into the LLM, which generates a text response based on both its somewhat out-of-date generalized knowledge and the extremely timely contextual information. Interestingly, while the process of training the generalized LLM is time-consuming and costly, updates to the RAG model are just the opposite. New data can be loaded into the embedded language model and translated into vectors on a continuous, incremental basis. In fact, the answers from the entire generative AI system can be fed back into the RAG model, improving its performance and accuracy, because, in effect, it knows how it has already answered a similar question. An additional benefit of RAG is that by using the vector database, the generative AI can provide the specific source of data cited in its answer—something LLMs can’t do. Therefore, if there’s an inaccuracy in the generative AI’s output, the document that contains that erroneous information can be quickly identified and corrected, and then the corrected information can be fed into the vector database. In short, RAG provides timeliness, context, and accuracy grounded in evidence to generative AI, going beyond what the LLM itself can provide. Retrieval-Augmented Generation vs. Semantic Search  RAG isn’t the only technique used to improve the accuracy of LLM-based generative AI. Another technique is semantic search, which helps the AI system narrow down the meaning of a query by seeking deep understanding of the specific words and phrases in the prompt.  Traditional search is focused on keywords. For example, a basic query asking about the tree species native to France might search the AI system’s database using “trees” and “France” as keywords and find data that contains both keywords—but the system might not truly comprehend the meaning of trees in France and therefore may retrieve too much information, too little, or even the wrong information. That keyword-based search might also miss information because the keyword search is too literal: The trees native to Normandy might be missed, even though they’re in France, because that keyword was missing.  Semantic search goes beyond keyword search by determining the meaning of questions and source documents and using that meaning to retrieve more accurate results. Semantic search is an integral part of RAG. Using RAG in Chat Applications When a person wants an instant answer to a question, it’s hard to beat the immediacy and usability of a chatbot. Most bots are trained on a finite number of intents—that is, the customer’s desired tasks or outcomes—and they respond to those intents. RAG capabilities can make current bots better by allowing the AI system to provide natural language answers to questions that aren’t in the intent list.  The “ask a question, get an answer” paradigm makes chatbots a perfect use case for generative AI, for many reasons. Questions often require specific context to generate an accurate answer, and given that chatbot users’ expectations about relevance and accuracy are often high, it’s clear how RAG techniques apply. In fact, for many organizations, chatbots may indeed be the starting point for RAG and generative AI use. Questions often require specific context to deliver an accurate answer. Customer queries about a newly introduced product, for example, aren’t useful if the data pertains to the previous model and may in fact be misleading. And a hiker who wants to know if a park is open this Sunday expects timely, accurate information about that specific park on that specific date. Benefits of Retrieval-Augmented Generation RAG techniques can be used to improve the quality of a generative AI system’s responses to prompts, beyond what an LLM alone can deliver. Benefits include the following: The RAG has access to information that may be fresher than the data used to train the LLM. Data in the RAG’s knowledge repository can be continually updated without incurring significant costs. The RAG’s knowledge repository can contain data that’s more contextual than the data in a generalized LLM. The source of the information in the RAG’s vector database can be identified. And because the data sources are known, incorrect information in the RAG can be corrected or deleted. Challenges of Retrieval-Augmented Generation Because RAG is a relatively new technology, first proposed in 2020, AI developers are still learning how to best implement its information retrieval mechanisms in generative AI. Some key challenges are Improving organizational knowledge and understanding of RAG because it’s so new Increasing costs; while generative AI with RAG will be more expensive to implement than an LLM on its own, this route is less costly than frequently retraining the LLM itself Determining how to best model the structured and unstructured data within the knowledge library and vector database Developing requirements for a process to incrementally feed data into the RAG system Putting processes in place to handle reports of inaccuracies and to correct or delete those information sources in the RAG system Examples of Retrieval-Augmented Generation There are many possible examples of generative AI augmented by RAG. Cohere, a leader in the field of generative AI and RAG, has written about a chatbot that can provide contextual information about a vacation rental in the Canary Islands, including fact-based answers about beach accessibility, lifeguards on nearby beaches, and the availability of volleyball courts within walking distance. Oracle has described other use cases for RAG, such as analyzing financial reports, assisting with gas and oil discovery, reviewing transcripts from call center customer exchanges, and searching medical databases for relevant research papers. Future of Retrieval-Augmented Generation Today, in the early phases of RAG, the technology is being used to provide timely, accurate, and contextual responses to queries. These use cases are appropriate to chatbots, email, text messaging, and other conversational applications. In the future, possible directions for RAG technology would be to help generative AI take an appropriate action based on contextual information and user prompts. For example, a RAG-augmented AI system might identify the highest-rated beach vacation rental on the Canary Islands and then initiate booking a two-bedroom cabin within walking distance of the beach during a volleyball tournament. RAG might also be able to assist with more sophisticated lines of questioning. Today, generative AI might be able to tell an employee about the company’s tuition reimbursement policy; RAG could add more contextual data to tell the employee which nearby schools have courses that fit into that policy and perhaps recommend programs that are suited to the employee’s jobs and previous training—maybe even help apply for those programs and initiate a reimbursement request. Generative AI With Oracle Oracle offers a variety of advanced cloud-based AI services, including the OCI Generative AI service running on Oracle Cloud Infrastructure (OCI). Oracle’s offerings include robust models based on your organization’s unique data and industry knowledge. Customer data is not shared with LLM providers or seen by other customers, and custom models trained on customer data can only be used by that customer. In addition, Oracle is integrating generative AI across its wide range of cloud applications, and generative AI capabilities are available to developers who use OCI and across its database portfolio. What’s more, Oracle’s AI services offer predictable performance and pricing using single-tenant AI clusters dedicated to your use. The power and capabilities of LLMs and generative AI are widely known and understood—they’ve been the subject of breathless news headlines for the past year. Retrieval-augmented generation builds on the benefits of LLMs by making them more timely, more accurate, and more contextual. For business applications of generative AI, RAG is an important technology to watch, study, and pilot. Oracle offers a modern data platform and low-cost, high-performance AI infrastructure. Additional factors, such as powerful, high-performing models, unrivaled data security, and embedded AI services demonstrate why Oracle’s AI offering is truly built for enterprises. Retrieval-Augmented Generation FAQs Is RAG the same as generative AI? No. Retrieval-augmented generation is a technique that can provide more accurate results to queries than a generative large language model on its own because RAG uses knowledge external to data already contained in the LLM. What type of information is used in RAG? RAG can incorporate data from many sources, such as relational databases, unstructured document repositories, internet data streams, media newsfeeds, audio transcripts, and transaction logs. How does generative AI use RAG? Data from enterprise data sources is embedded into a knowledge repository and then converted to vectors, which are stored in a vector database. When an end user makes a query, the vector database retrieves relevant contextual information. This contextual information, along with the query, is sent to the large language model, which uses the context to create a more timely, accurate, and contextual response. Can a RAG cite references for the data it retrieves? Yes. The vector databases and knowledge repositories used by RAG contain specific information about the sources of information. This means that sources can be cited, and if there’s an error in one of those sources it can be quickly corrected or deleted so that subsequent queries won’t return that incorrect information.']"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ruAwMmPbN2_",
        "outputId": "0a9c1008-a3e7-4554-d89b-2541c16ab119"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts[0]), len(texts[1]), len(texts[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLAKCScYYYOs",
        "outputId": "a9719765-100c-4a10-c94f-d769e07a1ae9"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7083, 10109, 14907)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P1rpe-fHH5H"
      },
      "source": [
        "# Cleaning/Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "Z09FAPueHH5I"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "texts[0] = clean_text(texts[0])\n",
        "texts[1] = clean_text(texts[1])\n",
        "texts[2] = clean_text(texts[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc4jKQu2bLZi",
        "outputId": "4cec1ec3-2195-4ca0-a616-cbb54497875f"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What is Retrieval-Augmented Generation (RAG)? RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs). By combining your data and world knowledge with LLM language skills, grounded generation is more accurate, up-to-date, and relevant to your specific needs. Check out this e-book to unlock your “Enterprise Truth.” How does Retrieval-Augmented Generation work? RAGs operate with a few main steps to help enhance generative AI outputs: Retrieval and pre-processing: RAGs leverage powerful search algorithms to query external data, such as web pages, knowledge bases, and databases. Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words. Grounded generation: The pre-processed retrieved information is then seamlessly incorporated into the pre-trained LLM. This integration enhances the LLM\\'s context, providing it with a more comprehensive understanding of the topic. This augmented context enables the LLM to generate more precise, informative, and engaging responses. Why Use RAG? RAG offers several advantages augmenting traditional methods of text generation, especially when dealing with factual information or data-driven responses. Here are some key reasons why using RAG can be beneficial: Access to fresh information LLMs are limited to their pre-trained data. This leads to outdated and potentially inaccurate responses. RAG overcomes this by providing up-to-date information to LLMs. Factual grounding LLMs are powerful tools for generating creative and engaging text, but they can sometimes struggle with factual accuracy. This is because LLMs are trained on massive amounts of text data, which may contain inaccuracies or biases. Providing “facts” to the LLM as part of the input prompt can mitigate “gen AI hallucinations.” The crux of this approach is ensuring that the most relevant facts are provided to the LLM, and that the LLM output is entirely grounded on those facts while also answering the user’s question and adhering to system instructions and safety constraints. Using Gemini’s long context window (LCW) is a great way to provide source materials to the LLM. If you need to provide more information than fits into the LCW, or if you need to scale up performance, you can use a RAG approach that will reduce the number of tokens, saving you time and cost. Search with vector databases and relevancy re-rankers RAGs usually retrieve facts via search, and modern search engines now leverage vector databases to efficiently retrieve relevant documents. Vector databases store documents as embeddings in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity. Multi-modal embeddings can be used for images, audio and video, and more and these media embeddings can be retrieved alongside text embeddings or multi-language embeddings. Advanced search engines like Vertex AI Search use semantic search and keyword search together (called hybrid search), and a re-ranker which scores search results to ensure the top returned results are the most relevant. Additionally searches perform better with a clear, focused query without misspellings; so prior to lookup, sophisticated search engines will transform a query and fix spelling mistakes. Relevance, accuracy, and quality The retrieval mechanism in RAG is critically important. You need the best semantic search on top of a curated knowledge base to ensure that the retrieved information is relevant to the input query or context. If your retrieved information is irrelevant, your generation could be grounded but off-topic or incorrect. By fine-tuning or prompt-engineering the LLM to generate text entirely based on the retrieved knowledge, RAG helps to minimize contradictions and inconsistencies in the generated text. This significantly improves the quality of the generated text, and improves the user experience. The Vertex Eval Service now scores LLM generated text and retrieved chunks on metrics like “coherence,” “fluency,” “groundedness,” \"safety,\" “instruction_following,” “question_answering_quality,” and more. These metrics help you measure the grounded text you get from the LLM (for some metrics that is a comparison to a ground truth answer you have provided). Implementing these evaluations gives you a baseline measurement and you can optimize for RAG quality by configuring your search engine, curating your source data, improving source layout parsing or chunking strategies, or refining the user’s question prior to search. A RAG Ops, metrics driven approach like this will help you hill climb to high quality RAG and grounded generation. RAGs, agents, and chatbots RAG and grounding can be integrated into any LLM application or agent which needs access to fresh, private, or specialized data. By accessing external information, RAG-powered chatbots and conversational agents leverage external knowledge to provide more comprehensive, informative, and context-aware responses, improving the overall user experience. Your data and your use case are what differentiate what you are building with gen AI. RAG and grounding bring your data to LLMs efficiently and scalably. What Google Cloud products and services are related to RAG? The following Google Cloud products are related to Retrieval-Augmented Generation: Vertex AI SearchVertex AI Search is Google Search for your data, a fully managed, out-of-the-box search and RAG builder. Vertex AI Vector SearchThe ultra performant vector index that powers Vertex AI Search; it enables semantic and hybrid search and retrieval from huge collections of embeddings with high recall at high query rate. BigQueryLarge datasets that you can use to train machine learning models, including models for Vertex AI Vector Search. Grounded Generation APIGemini high-fidelity mode grounded with Google Search or inline facts or bring your own search engine. AlloyDBRun models in Vertex AI and access them in your application using familiar SQL queries. Use Google models, such as Gemini, or your own custom models. LlamaIndex on VertexBuild your own search engine for RAG and grounding using Google or open source components and our fully managed orchestration system based on LlamaIndex. Learn more about using retrieval augmented generation with these resources. Using Vertex AI to build next-gen search applications | Google Cloud Blog RAGs powered by Google Search technology RAG with databases on Google Cloud Infrastructure for a RAG-capable generative AI application using Vertex AI APIs to build your own search and Retrieval Augmented Generation (RAG) systems How to use RAG in BigQuery to bolster LLMs Code sample and quickstart to get familiar with RAG Start building on Google Cloud with $300 in free credits and 20+ always free products. Need help getting started?Contact sales Work with a trusted partnerFind a partner Continue browsingSee all products',\n",
              " 'What is Cloud Computing? Cloud Computing Concepts Hub Artificial Intelligence Generative AI What is RAG (Retrieval-Augmented Generation)? What is Retrieval-Augmented Generation? Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization\\'s internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. Why is Retrieval-Augmented Generation important? LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has. Known challenges of LLMs include: Presenting false information when it does not have the answer. Presenting out-of-date or generic information when the user expects a specific, current response. Creating a response from non-authoritative sources. Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things. You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate! RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response. What are the benefits of Retrieval-Augmented Generation? RAG technology brings several benefits to an organization\\'s generative AI efforts. Cost-effective implementation Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable. Current information Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users. Enhanced user trust RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution. More developer control With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM\\'s information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications. How does Retrieval-Augmented Generation work? Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process. Create external data The new data outside of the LLM\\'s original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand. Retrieve relevant information The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee\\'s past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations. Augment the LLM prompt Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries. Update external data The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used. The following diagram shows the conceptual flow of using RAG with LLMs. What is the difference between Retrieval-Augmented Generation and semantic search? Semantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications. Modern enterprises store vast amounts of information like manuals, FAQs, research reports, customer service guides, and human resource document repositories across various systems. Context retrieval is challenging at scale and consequently lowers generative output quality. Semantic search technologies can scan large databases of disparate information and retrieve data more accurately. For example, they can answer questions such as, \"How much was spent on machinery repairs last year?” by mapping the question to the relevant documents and returning specific text instead of search results. Developers can then use that answer to provide more context to the LLM. Conventional or keyword search solutions in RAG produce limited results for knowledge-intensive tasks. Developers must also deal with word embeddings, document chunking, and other complexities as they manually prepare their data. In contrast, semantic search technologies do all the work of knowledge base preparation so developers don\\'t have to. They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload. How can AWS support your Retrieval-Augmented Generation requirements? Amazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models—along with a broad set of capabilities—to build generative AI applications while simplifying development and maintaining privacy and security. With knowledge bases for Amazon Bedrock, you can connect FMs to your data sources for RAG in just a few clicks. Vector conversions, retrievals, and improved output generation are all handled automatically. For organizations managing their own RAG, Amazon Kendra is a highly-accurate enterprise search service powered by machine learning. It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra’s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows. For example, with the Retrieve API, you can: Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance. Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites. Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files. Filter responses based on those documents that the end-user permissions allow. Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples. Get started with Retrieval-Augmented Generation on AWS by creating a free account today Next Steps on AWS Instant get access to the AWS Free Tier. Get started building in the AWS management console.',\n",
              " 'Accessibility Policy Skip to content QUICK LINKS Oracle Cloud Infrastructure Oracle Fusion Cloud Applications Oracle Database Download Java Careers at Oracle What Is Retrieval-Augmented Generation (RAG)? Alan Zeichick | Tech Content Strategist | September 19, 2023 In This Article What Is Retrieval-Augmented Generation (RAG)? Retrieval-Augmented Generation Explained How Does Retrieval-Augmented Generation Work? Using RAG in Chat Applications Benefits of Retrieval-Augmented Generation Challenges of Retrieval-Augmented Generation Examples of Retrieval-Augmented Generation Future of Retrieval-Augmented Generation Generative AI With Oracle Retrieval-Augmented Generation FAQs Generative artificial intelligence (AI) excels at creating text responses based on large language models (LLMs) where the AI is trained on a massive number of data points. The good news is that the generated text is often easy to read and provides detailed responses that are broadly applicable to the questions asked of the software, often called prompts. The bad news is that the information used to generate the response is limited to the information used to train the AI, often a generalized LLM. The LLM’s data may be weeks, months, or years out of date and in a corporate AI chatbot may not include specific information about the organization’s products or services. That can lead to incorrect responses that erode confidence in the technology among customers and employees. What Is Retrieval-Augmented Generation (RAG)? That’s where retrieval-augmented generation (RAG) comes in. RAG provides a way to optimize the output of an LLM with targeted information without modifying the underlying model itself; that targeted information can be more up-to-date than the LLM as well as specific to a particular organization and industry. That means the generative AI system can provide more contextually appropriate answers to prompts as well as base those answers on extremely current data. RAG first came to the attention of generative AI developers after the publication of “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” a 2020 paper published by Patrick Lewis and a team at Facebook AI Research. The RAG concept has been embraced by many academic and industry researchers, who see it as a way to significantly improve the value of generative AI systems. Retrieval-Augmented Generation Explained Consider a sports league that wants fans and the media to be able to use chat to access its data and answer questions about players, teams, the sport’s history and rules, and current stats and standings. A generalized LLM could answer questions about the history and rules or perhaps describe a particular team’s stadium. It wouldn’t be able to discuss last night’s game or provide current information about a particular athlete’s injury because the LLM wouldn’t have that information—and given that an LLM takes significant computing horsepower to retrain, it isn’t feasible to keep the model current. In addition to the large, fairly static LLM, the sports league owns or can access many other information sources, including databases, data warehouses, documents containing player bios, and news feeds that discuss each game in depth. RAG lets the generative AI ingest this information. Now, the chat can provide information that’s more timely, more contextually appropriate, and more accurate. Simply put, RAG helps LLMs give better answers. Key Takeaways RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining. RAG models build knowledge repositories based on the organization’s own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers. Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI. Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM. How Does Retrieval-Augmented Generation Work? Consider all the information that an organization has—the structured databases, the unstructured PDFs and other documents, the blogs, the news feeds, the chat transcripts from past customer service sessions. In RAG, this vast quantity of dynamic data is translated into a common format and stored in a knowledge library that’s accessible to the generative AI system. The data in that knowledge library is then processed into numerical representations using a special type of algorithm called an embedded language model and stored in a vector database, which can be quickly searched and used to retrieve the correct contextual information. RAG and Large Language Models (LLMs) Now, say an end user sends the generative AI system a specific prompt, for example, “Where will tonight’s game be played, who are the starting players, and what are reporters saying about the matchup?” The query is transformed into a vector and used to query the vector database, which retrieves information relevant to that question’s context. That contextual information plus the original prompt are then fed into the LLM, which generates a text response based on both its somewhat out-of-date generalized knowledge and the extremely timely contextual information. Interestingly, while the process of training the generalized LLM is time-consuming and costly, updates to the RAG model are just the opposite. New data can be loaded into the embedded language model and translated into vectors on a continuous, incremental basis. In fact, the answers from the entire generative AI system can be fed back into the RAG model, improving its performance and accuracy, because, in effect, it knows how it has already answered a similar question. An additional benefit of RAG is that by using the vector database, the generative AI can provide the specific source of data cited in its answer—something LLMs can’t do. Therefore, if there’s an inaccuracy in the generative AI’s output, the document that contains that erroneous information can be quickly identified and corrected, and then the corrected information can be fed into the vector database. In short, RAG provides timeliness, context, and accuracy grounded in evidence to generative AI, going beyond what the LLM itself can provide. Retrieval-Augmented Generation vs. Semantic Search RAG isn’t the only technique used to improve the accuracy of LLM-based generative AI. Another technique is semantic search, which helps the AI system narrow down the meaning of a query by seeking deep understanding of the specific words and phrases in the prompt. Traditional search is focused on keywords. For example, a basic query asking about the tree species native to France might search the AI system’s database using “trees” and “France” as keywords and find data that contains both keywords—but the system might not truly comprehend the meaning of trees in France and therefore may retrieve too much information, too little, or even the wrong information. That keyword-based search might also miss information because the keyword search is too literal: The trees native to Normandy might be missed, even though they’re in France, because that keyword was missing. Semantic search goes beyond keyword search by determining the meaning of questions and source documents and using that meaning to retrieve more accurate results. Semantic search is an integral part of RAG. Using RAG in Chat Applications When a person wants an instant answer to a question, it’s hard to beat the immediacy and usability of a chatbot. Most bots are trained on a finite number of intents—that is, the customer’s desired tasks or outcomes—and they respond to those intents. RAG capabilities can make current bots better by allowing the AI system to provide natural language answers to questions that aren’t in the intent list. The “ask a question, get an answer” paradigm makes chatbots a perfect use case for generative AI, for many reasons. Questions often require specific context to generate an accurate answer, and given that chatbot users’ expectations about relevance and accuracy are often high, it’s clear how RAG techniques apply. In fact, for many organizations, chatbots may indeed be the starting point for RAG and generative AI use. Questions often require specific context to deliver an accurate answer. Customer queries about a newly introduced product, for example, aren’t useful if the data pertains to the previous model and may in fact be misleading. And a hiker who wants to know if a park is open this Sunday expects timely, accurate information about that specific park on that specific date. Benefits of Retrieval-Augmented Generation RAG techniques can be used to improve the quality of a generative AI system’s responses to prompts, beyond what an LLM alone can deliver. Benefits include the following: The RAG has access to information that may be fresher than the data used to train the LLM. Data in the RAG’s knowledge repository can be continually updated without incurring significant costs. The RAG’s knowledge repository can contain data that’s more contextual than the data in a generalized LLM. The source of the information in the RAG’s vector database can be identified. And because the data sources are known, incorrect information in the RAG can be corrected or deleted. Challenges of Retrieval-Augmented Generation Because RAG is a relatively new technology, first proposed in 2020, AI developers are still learning how to best implement its information retrieval mechanisms in generative AI. Some key challenges are Improving organizational knowledge and understanding of RAG because it’s so new Increasing costs; while generative AI with RAG will be more expensive to implement than an LLM on its own, this route is less costly than frequently retraining the LLM itself Determining how to best model the structured and unstructured data within the knowledge library and vector database Developing requirements for a process to incrementally feed data into the RAG system Putting processes in place to handle reports of inaccuracies and to correct or delete those information sources in the RAG system Examples of Retrieval-Augmented Generation There are many possible examples of generative AI augmented by RAG. Cohere, a leader in the field of generative AI and RAG, has written about a chatbot that can provide contextual information about a vacation rental in the Canary Islands, including fact-based answers about beach accessibility, lifeguards on nearby beaches, and the availability of volleyball courts within walking distance. Oracle has described other use cases for RAG, such as analyzing financial reports, assisting with gas and oil discovery, reviewing transcripts from call center customer exchanges, and searching medical databases for relevant research papers. Future of Retrieval-Augmented Generation Today, in the early phases of RAG, the technology is being used to provide timely, accurate, and contextual responses to queries. These use cases are appropriate to chatbots, email, text messaging, and other conversational applications. In the future, possible directions for RAG technology would be to help generative AI take an appropriate action based on contextual information and user prompts. For example, a RAG-augmented AI system might identify the highest-rated beach vacation rental on the Canary Islands and then initiate booking a two-bedroom cabin within walking distance of the beach during a volleyball tournament. RAG might also be able to assist with more sophisticated lines of questioning. Today, generative AI might be able to tell an employee about the company’s tuition reimbursement policy; RAG could add more contextual data to tell the employee which nearby schools have courses that fit into that policy and perhaps recommend programs that are suited to the employee’s jobs and previous training—maybe even help apply for those programs and initiate a reimbursement request. Generative AI With Oracle Oracle offers a variety of advanced cloud-based AI services, including the OCI Generative AI service running on Oracle Cloud Infrastructure (OCI). Oracle’s offerings include robust models based on your organization’s unique data and industry knowledge. Customer data is not shared with LLM providers or seen by other customers, and custom models trained on customer data can only be used by that customer. In addition, Oracle is integrating generative AI across its wide range of cloud applications, and generative AI capabilities are available to developers who use OCI and across its database portfolio. What’s more, Oracle’s AI services offer predictable performance and pricing using single-tenant AI clusters dedicated to your use. The power and capabilities of LLMs and generative AI are widely known and understood—they’ve been the subject of breathless news headlines for the past year. Retrieval-augmented generation builds on the benefits of LLMs by making them more timely, more accurate, and more contextual. For business applications of generative AI, RAG is an important technology to watch, study, and pilot. Oracle offers a modern data platform and low-cost, high-performance AI infrastructure. Additional factors, such as powerful, high-performing models, unrivaled data security, and embedded AI services demonstrate why Oracle’s AI offering is truly built for enterprises. Retrieval-Augmented Generation FAQs Is RAG the same as generative AI? No. Retrieval-augmented generation is a technique that can provide more accurate results to queries than a generative large language model on its own because RAG uses knowledge external to data already contained in the LLM. What type of information is used in RAG? RAG can incorporate data from many sources, such as relational databases, unstructured document repositories, internet data streams, media newsfeeds, audio transcripts, and transaction logs. How does generative AI use RAG? Data from enterprise data sources is embedded into a knowledge repository and then converted to vectors, which are stored in a vector database. When an end user makes a query, the vector database retrieves relevant contextual information. This contextual information, along with the query, is sent to the large language model, which uses the context to create a more timely, accurate, and contextual response. Can a RAG cite references for the data it retrieves? Yes. The vector databases and knowledge repositories used by RAG contain specific information about the sources of information. This means that sources can be cited, and if there’s an error in one of those sources it can be quickly corrected or deleted so that subsequent queries won’t return that incorrect information.']"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLXJQdcdHH5I",
        "outputId": "a84d0cd8-8c8c-4b1e-e8ec-221f2dfc955c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7081, 10105, 14900)"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ],
      "source": [
        "len(texts[0]), len(texts[1]), len(texts[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_text = \" \".join(texts)\n",
        "final_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "mku22u_ZbuF-",
        "outputId": "765b908a-75d8-4134-9cfb-cc1a4c2852f6",
        "collapsed": true
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is Retrieval-Augmented Generation (RAG)? RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs). By combining your data and world knowledge with LLM language skills, grounded generation is more accurate, up-to-date, and relevant to your specific needs. Check out this e-book to unlock your “Enterprise Truth.” How does Retrieval-Augmented Generation work? RAGs operate with a few main steps to help enhance generative AI outputs: Retrieval and pre-processing: RAGs leverage powerful search algorithms to query external data, such as web pages, knowledge bases, and databases. Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words. Grounded generation: The pre-processed retrieved information is then seamlessly incorporated into the pre-trained LLM. This integration enhances the LLM\\'s context, providing it with a more comprehensive understanding of the topic. This augmented context enables the LLM to generate more precise, informative, and engaging responses. Why Use RAG? RAG offers several advantages augmenting traditional methods of text generation, especially when dealing with factual information or data-driven responses. Here are some key reasons why using RAG can be beneficial: Access to fresh information LLMs are limited to their pre-trained data. This leads to outdated and potentially inaccurate responses. RAG overcomes this by providing up-to-date information to LLMs. Factual grounding LLMs are powerful tools for generating creative and engaging text, but they can sometimes struggle with factual accuracy. This is because LLMs are trained on massive amounts of text data, which may contain inaccuracies or biases. Providing “facts” to the LLM as part of the input prompt can mitigate “gen AI hallucinations.” The crux of this approach is ensuring that the most relevant facts are provided to the LLM, and that the LLM output is entirely grounded on those facts while also answering the user’s question and adhering to system instructions and safety constraints. Using Gemini’s long context window (LCW) is a great way to provide source materials to the LLM. If you need to provide more information than fits into the LCW, or if you need to scale up performance, you can use a RAG approach that will reduce the number of tokens, saving you time and cost. Search with vector databases and relevancy re-rankers RAGs usually retrieve facts via search, and modern search engines now leverage vector databases to efficiently retrieve relevant documents. Vector databases store documents as embeddings in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity. Multi-modal embeddings can be used for images, audio and video, and more and these media embeddings can be retrieved alongside text embeddings or multi-language embeddings. Advanced search engines like Vertex AI Search use semantic search and keyword search together (called hybrid search), and a re-ranker which scores search results to ensure the top returned results are the most relevant. Additionally searches perform better with a clear, focused query without misspellings; so prior to lookup, sophisticated search engines will transform a query and fix spelling mistakes. Relevance, accuracy, and quality The retrieval mechanism in RAG is critically important. You need the best semantic search on top of a curated knowledge base to ensure that the retrieved information is relevant to the input query or context. If your retrieved information is irrelevant, your generation could be grounded but off-topic or incorrect. By fine-tuning or prompt-engineering the LLM to generate text entirely based on the retrieved knowledge, RAG helps to minimize contradictions and inconsistencies in the generated text. This significantly improves the quality of the generated text, and improves the user experience. The Vertex Eval Service now scores LLM generated text and retrieved chunks on metrics like “coherence,” “fluency,” “groundedness,” \"safety,\" “instruction_following,” “question_answering_quality,” and more. These metrics help you measure the grounded text you get from the LLM (for some metrics that is a comparison to a ground truth answer you have provided). Implementing these evaluations gives you a baseline measurement and you can optimize for RAG quality by configuring your search engine, curating your source data, improving source layout parsing or chunking strategies, or refining the user’s question prior to search. A RAG Ops, metrics driven approach like this will help you hill climb to high quality RAG and grounded generation. RAGs, agents, and chatbots RAG and grounding can be integrated into any LLM application or agent which needs access to fresh, private, or specialized data. By accessing external information, RAG-powered chatbots and conversational agents leverage external knowledge to provide more comprehensive, informative, and context-aware responses, improving the overall user experience. Your data and your use case are what differentiate what you are building with gen AI. RAG and grounding bring your data to LLMs efficiently and scalably. What Google Cloud products and services are related to RAG? The following Google Cloud products are related to Retrieval-Augmented Generation: Vertex AI SearchVertex AI Search is Google Search for your data, a fully managed, out-of-the-box search and RAG builder. Vertex AI Vector SearchThe ultra performant vector index that powers Vertex AI Search; it enables semantic and hybrid search and retrieval from huge collections of embeddings with high recall at high query rate. BigQueryLarge datasets that you can use to train machine learning models, including models for Vertex AI Vector Search. Grounded Generation APIGemini high-fidelity mode grounded with Google Search or inline facts or bring your own search engine. AlloyDBRun models in Vertex AI and access them in your application using familiar SQL queries. Use Google models, such as Gemini, or your own custom models. LlamaIndex on VertexBuild your own search engine for RAG and grounding using Google or open source components and our fully managed orchestration system based on LlamaIndex. Learn more about using retrieval augmented generation with these resources. Using Vertex AI to build next-gen search applications | Google Cloud Blog RAGs powered by Google Search technology RAG with databases on Google Cloud Infrastructure for a RAG-capable generative AI application using Vertex AI APIs to build your own search and Retrieval Augmented Generation (RAG) systems How to use RAG in BigQuery to bolster LLMs Code sample and quickstart to get familiar with RAG Start building on Google Cloud with $300 in free credits and 20+ always free products. Need help getting started?Contact sales Work with a trusted partnerFind a partner Continue browsingSee all products What is Cloud Computing? Cloud Computing Concepts Hub Artificial Intelligence Generative AI What is RAG (Retrieval-Augmented Generation)? What is Retrieval-Augmented Generation? Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization\\'s internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. Why is Retrieval-Augmented Generation important? LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has. Known challenges of LLMs include: Presenting false information when it does not have the answer. Presenting out-of-date or generic information when the user expects a specific, current response. Creating a response from non-authoritative sources. Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things. You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate! RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response. What are the benefits of Retrieval-Augmented Generation? RAG technology brings several benefits to an organization\\'s generative AI efforts. Cost-effective implementation Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable. Current information Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users. Enhanced user trust RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution. More developer control With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM\\'s information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications. How does Retrieval-Augmented Generation work? Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process. Create external data The new data outside of the LLM\\'s original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand. Retrieve relevant information The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee\\'s past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations. Augment the LLM prompt Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries. Update external data The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used. The following diagram shows the conceptual flow of using RAG with LLMs. What is the difference between Retrieval-Augmented Generation and semantic search? Semantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications. Modern enterprises store vast amounts of information like manuals, FAQs, research reports, customer service guides, and human resource document repositories across various systems. Context retrieval is challenging at scale and consequently lowers generative output quality. Semantic search technologies can scan large databases of disparate information and retrieve data more accurately. For example, they can answer questions such as, \"How much was spent on machinery repairs last year?” by mapping the question to the relevant documents and returning specific text instead of search results. Developers can then use that answer to provide more context to the LLM. Conventional or keyword search solutions in RAG produce limited results for knowledge-intensive tasks. Developers must also deal with word embeddings, document chunking, and other complexities as they manually prepare their data. In contrast, semantic search technologies do all the work of knowledge base preparation so developers don\\'t have to. They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload. How can AWS support your Retrieval-Augmented Generation requirements? Amazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models—along with a broad set of capabilities—to build generative AI applications while simplifying development and maintaining privacy and security. With knowledge bases for Amazon Bedrock, you can connect FMs to your data sources for RAG in just a few clicks. Vector conversions, retrievals, and improved output generation are all handled automatically. For organizations managing their own RAG, Amazon Kendra is a highly-accurate enterprise search service powered by machine learning. It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra’s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows. For example, with the Retrieve API, you can: Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance. Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites. Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files. Filter responses based on those documents that the end-user permissions allow. Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples. Get started with Retrieval-Augmented Generation on AWS by creating a free account today Next Steps on AWS Instant get access to the AWS Free Tier. Get started building in the AWS management console. Accessibility Policy Skip to content QUICK LINKS Oracle Cloud Infrastructure Oracle Fusion Cloud Applications Oracle Database Download Java Careers at Oracle What Is Retrieval-Augmented Generation (RAG)? Alan Zeichick | Tech Content Strategist | September 19, 2023 In This Article What Is Retrieval-Augmented Generation (RAG)? Retrieval-Augmented Generation Explained How Does Retrieval-Augmented Generation Work? Using RAG in Chat Applications Benefits of Retrieval-Augmented Generation Challenges of Retrieval-Augmented Generation Examples of Retrieval-Augmented Generation Future of Retrieval-Augmented Generation Generative AI With Oracle Retrieval-Augmented Generation FAQs Generative artificial intelligence (AI) excels at creating text responses based on large language models (LLMs) where the AI is trained on a massive number of data points. The good news is that the generated text is often easy to read and provides detailed responses that are broadly applicable to the questions asked of the software, often called prompts. The bad news is that the information used to generate the response is limited to the information used to train the AI, often a generalized LLM. The LLM’s data may be weeks, months, or years out of date and in a corporate AI chatbot may not include specific information about the organization’s products or services. That can lead to incorrect responses that erode confidence in the technology among customers and employees. What Is Retrieval-Augmented Generation (RAG)? That’s where retrieval-augmented generation (RAG) comes in. RAG provides a way to optimize the output of an LLM with targeted information without modifying the underlying model itself; that targeted information can be more up-to-date than the LLM as well as specific to a particular organization and industry. That means the generative AI system can provide more contextually appropriate answers to prompts as well as base those answers on extremely current data. RAG first came to the attention of generative AI developers after the publication of “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” a 2020 paper published by Patrick Lewis and a team at Facebook AI Research. The RAG concept has been embraced by many academic and industry researchers, who see it as a way to significantly improve the value of generative AI systems. Retrieval-Augmented Generation Explained Consider a sports league that wants fans and the media to be able to use chat to access its data and answer questions about players, teams, the sport’s history and rules, and current stats and standings. A generalized LLM could answer questions about the history and rules or perhaps describe a particular team’s stadium. It wouldn’t be able to discuss last night’s game or provide current information about a particular athlete’s injury because the LLM wouldn’t have that information—and given that an LLM takes significant computing horsepower to retrain, it isn’t feasible to keep the model current. In addition to the large, fairly static LLM, the sports league owns or can access many other information sources, including databases, data warehouses, documents containing player bios, and news feeds that discuss each game in depth. RAG lets the generative AI ingest this information. Now, the chat can provide information that’s more timely, more contextually appropriate, and more accurate. Simply put, RAG helps LLMs give better answers. Key Takeaways RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining. RAG models build knowledge repositories based on the organization’s own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers. Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI. Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM. How Does Retrieval-Augmented Generation Work? Consider all the information that an organization has—the structured databases, the unstructured PDFs and other documents, the blogs, the news feeds, the chat transcripts from past customer service sessions. In RAG, this vast quantity of dynamic data is translated into a common format and stored in a knowledge library that’s accessible to the generative AI system. The data in that knowledge library is then processed into numerical representations using a special type of algorithm called an embedded language model and stored in a vector database, which can be quickly searched and used to retrieve the correct contextual information. RAG and Large Language Models (LLMs) Now, say an end user sends the generative AI system a specific prompt, for example, “Where will tonight’s game be played, who are the starting players, and what are reporters saying about the matchup?” The query is transformed into a vector and used to query the vector database, which retrieves information relevant to that question’s context. That contextual information plus the original prompt are then fed into the LLM, which generates a text response based on both its somewhat out-of-date generalized knowledge and the extremely timely contextual information. Interestingly, while the process of training the generalized LLM is time-consuming and costly, updates to the RAG model are just the opposite. New data can be loaded into the embedded language model and translated into vectors on a continuous, incremental basis. In fact, the answers from the entire generative AI system can be fed back into the RAG model, improving its performance and accuracy, because, in effect, it knows how it has already answered a similar question. An additional benefit of RAG is that by using the vector database, the generative AI can provide the specific source of data cited in its answer—something LLMs can’t do. Therefore, if there’s an inaccuracy in the generative AI’s output, the document that contains that erroneous information can be quickly identified and corrected, and then the corrected information can be fed into the vector database. In short, RAG provides timeliness, context, and accuracy grounded in evidence to generative AI, going beyond what the LLM itself can provide. Retrieval-Augmented Generation vs. Semantic Search RAG isn’t the only technique used to improve the accuracy of LLM-based generative AI. Another technique is semantic search, which helps the AI system narrow down the meaning of a query by seeking deep understanding of the specific words and phrases in the prompt. Traditional search is focused on keywords. For example, a basic query asking about the tree species native to France might search the AI system’s database using “trees” and “France” as keywords and find data that contains both keywords—but the system might not truly comprehend the meaning of trees in France and therefore may retrieve too much information, too little, or even the wrong information. That keyword-based search might also miss information because the keyword search is too literal: The trees native to Normandy might be missed, even though they’re in France, because that keyword was missing. Semantic search goes beyond keyword search by determining the meaning of questions and source documents and using that meaning to retrieve more accurate results. Semantic search is an integral part of RAG. Using RAG in Chat Applications When a person wants an instant answer to a question, it’s hard to beat the immediacy and usability of a chatbot. Most bots are trained on a finite number of intents—that is, the customer’s desired tasks or outcomes—and they respond to those intents. RAG capabilities can make current bots better by allowing the AI system to provide natural language answers to questions that aren’t in the intent list. The “ask a question, get an answer” paradigm makes chatbots a perfect use case for generative AI, for many reasons. Questions often require specific context to generate an accurate answer, and given that chatbot users’ expectations about relevance and accuracy are often high, it’s clear how RAG techniques apply. In fact, for many organizations, chatbots may indeed be the starting point for RAG and generative AI use. Questions often require specific context to deliver an accurate answer. Customer queries about a newly introduced product, for example, aren’t useful if the data pertains to the previous model and may in fact be misleading. And a hiker who wants to know if a park is open this Sunday expects timely, accurate information about that specific park on that specific date. Benefits of Retrieval-Augmented Generation RAG techniques can be used to improve the quality of a generative AI system’s responses to prompts, beyond what an LLM alone can deliver. Benefits include the following: The RAG has access to information that may be fresher than the data used to train the LLM. Data in the RAG’s knowledge repository can be continually updated without incurring significant costs. The RAG’s knowledge repository can contain data that’s more contextual than the data in a generalized LLM. The source of the information in the RAG’s vector database can be identified. And because the data sources are known, incorrect information in the RAG can be corrected or deleted. Challenges of Retrieval-Augmented Generation Because RAG is a relatively new technology, first proposed in 2020, AI developers are still learning how to best implement its information retrieval mechanisms in generative AI. Some key challenges are Improving organizational knowledge and understanding of RAG because it’s so new Increasing costs; while generative AI with RAG will be more expensive to implement than an LLM on its own, this route is less costly than frequently retraining the LLM itself Determining how to best model the structured and unstructured data within the knowledge library and vector database Developing requirements for a process to incrementally feed data into the RAG system Putting processes in place to handle reports of inaccuracies and to correct or delete those information sources in the RAG system Examples of Retrieval-Augmented Generation There are many possible examples of generative AI augmented by RAG. Cohere, a leader in the field of generative AI and RAG, has written about a chatbot that can provide contextual information about a vacation rental in the Canary Islands, including fact-based answers about beach accessibility, lifeguards on nearby beaches, and the availability of volleyball courts within walking distance. Oracle has described other use cases for RAG, such as analyzing financial reports, assisting with gas and oil discovery, reviewing transcripts from call center customer exchanges, and searching medical databases for relevant research papers. Future of Retrieval-Augmented Generation Today, in the early phases of RAG, the technology is being used to provide timely, accurate, and contextual responses to queries. These use cases are appropriate to chatbots, email, text messaging, and other conversational applications. In the future, possible directions for RAG technology would be to help generative AI take an appropriate action based on contextual information and user prompts. For example, a RAG-augmented AI system might identify the highest-rated beach vacation rental on the Canary Islands and then initiate booking a two-bedroom cabin within walking distance of the beach during a volleyball tournament. RAG might also be able to assist with more sophisticated lines of questioning. Today, generative AI might be able to tell an employee about the company’s tuition reimbursement policy; RAG could add more contextual data to tell the employee which nearby schools have courses that fit into that policy and perhaps recommend programs that are suited to the employee’s jobs and previous training—maybe even help apply for those programs and initiate a reimbursement request. Generative AI With Oracle Oracle offers a variety of advanced cloud-based AI services, including the OCI Generative AI service running on Oracle Cloud Infrastructure (OCI). Oracle’s offerings include robust models based on your organization’s unique data and industry knowledge. Customer data is not shared with LLM providers or seen by other customers, and custom models trained on customer data can only be used by that customer. In addition, Oracle is integrating generative AI across its wide range of cloud applications, and generative AI capabilities are available to developers who use OCI and across its database portfolio. What’s more, Oracle’s AI services offer predictable performance and pricing using single-tenant AI clusters dedicated to your use. The power and capabilities of LLMs and generative AI are widely known and understood—they’ve been the subject of breathless news headlines for the past year. Retrieval-augmented generation builds on the benefits of LLMs by making them more timely, more accurate, and more contextual. For business applications of generative AI, RAG is an important technology to watch, study, and pilot. Oracle offers a modern data platform and low-cost, high-performance AI infrastructure. Additional factors, such as powerful, high-performing models, unrivaled data security, and embedded AI services demonstrate why Oracle’s AI offering is truly built for enterprises. Retrieval-Augmented Generation FAQs Is RAG the same as generative AI? No. Retrieval-augmented generation is a technique that can provide more accurate results to queries than a generative large language model on its own because RAG uses knowledge external to data already contained in the LLM. What type of information is used in RAG? RAG can incorporate data from many sources, such as relational databases, unstructured document repositories, internet data streams, media newsfeeds, audio transcripts, and transaction logs. How does generative AI use RAG? Data from enterprise data sources is embedded into a knowledge repository and then converted to vectors, which are stored in a vector database. When an end user makes a query, the vector database retrieves relevant contextual information. This contextual information, along with the query, is sent to the large language model, which uses the context to create a more timely, accurate, and contextual response. Can a RAG cite references for the data it retrieves? Yes. The vector databases and knowledge repositories used by RAG contain specific information about the sources of information. This means that sources can be cited, and if there’s an error in one of those sources it can be quickly corrected or deleted so that subsequent queries won’t return that incorrect information.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tVooRvGc-0q",
        "outputId": "f7b8f98d-6875-4081-eefe-ebc9f478d095"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32088"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zj0rl_81wOn5"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWZsofpzHH5K"
      },
      "source": [
        "# Knowledge base using sentence embenddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import faiss # Facebook AI Similarity Seacrh\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score"
      ],
      "metadata": {
        "id": "XyPToldRbWpY"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Example corpus\n",
        "corpus = final_text\n",
        "sentences = sent_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J8Mzp4muhMDL",
        "outputId": "88062b29-0959-46c0-da08-906926dbce7e"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MAbfeQ8p5za",
        "outputId": "426b4e56-0bd8-4983-c07e-930b3c72cdfc"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What is Retrieval-Augmented Generation (RAG)?',\n",
              " 'RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs).',\n",
              " 'By combining your data and world knowledge with LLM language skills, grounded generation is more accurate, up-to-date, and relevant to your specific needs.',\n",
              " 'Check out this e-book to unlock your “Enterprise Truth.” How does Retrieval-Augmented Generation work?',\n",
              " 'RAGs operate with a few main steps to help enhance generative AI outputs: Retrieval and pre-processing: RAGs leverage powerful search algorithms to query external data, such as web pages, knowledge bases, and databases.',\n",
              " 'Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words.',\n",
              " 'Grounded generation: The pre-processed retrieved information is then seamlessly incorporated into the pre-trained LLM.',\n",
              " \"This integration enhances the LLM's context, providing it with a more comprehensive understanding of the topic.\",\n",
              " 'This augmented context enables the LLM to generate more precise, informative, and engaging responses.',\n",
              " 'Why Use RAG?',\n",
              " 'RAG offers several advantages augmenting traditional methods of text generation, especially when dealing with factual information or data-driven responses.',\n",
              " 'Here are some key reasons why using RAG can be beneficial: Access to fresh information LLMs are limited to their pre-trained data.',\n",
              " 'This leads to outdated and potentially inaccurate responses.',\n",
              " 'RAG overcomes this by providing up-to-date information to LLMs.',\n",
              " 'Factual grounding LLMs are powerful tools for generating creative and engaging text, but they can sometimes struggle with factual accuracy.',\n",
              " 'This is because LLMs are trained on massive amounts of text data, which may contain inaccuracies or biases.',\n",
              " 'Providing “facts” to the LLM as part of the input prompt can mitigate “gen AI hallucinations.” The crux of this approach is ensuring that the most relevant facts are provided to the LLM, and that the LLM output is entirely grounded on those facts while also answering the user’s question and adhering to system instructions and safety constraints.',\n",
              " 'Using Gemini’s long context window (LCW) is a great way to provide source materials to the LLM.',\n",
              " 'If you need to provide more information than fits into the LCW, or if you need to scale up performance, you can use a RAG approach that will reduce the number of tokens, saving you time and cost.',\n",
              " 'Search with vector databases and relevancy re-rankers RAGs usually retrieve facts via search, and modern search engines now leverage vector databases to efficiently retrieve relevant documents.',\n",
              " 'Vector databases store documents as embeddings in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity.',\n",
              " 'Multi-modal embeddings can be used for images, audio and video, and more and these media embeddings can be retrieved alongside text embeddings or multi-language embeddings.',\n",
              " 'Advanced search engines like Vertex AI Search use semantic search and keyword search together (called hybrid search), and a re-ranker which scores search results to ensure the top returned results are the most relevant.',\n",
              " 'Additionally searches perform better with a clear, focused query without misspellings; so prior to lookup, sophisticated search engines will transform a query and fix spelling mistakes.',\n",
              " 'Relevance, accuracy, and quality The retrieval mechanism in RAG is critically important.',\n",
              " 'You need the best semantic search on top of a curated knowledge base to ensure that the retrieved information is relevant to the input query or context.',\n",
              " 'If your retrieved information is irrelevant, your generation could be grounded but off-topic or incorrect.',\n",
              " 'By fine-tuning or prompt-engineering the LLM to generate text entirely based on the retrieved knowledge, RAG helps to minimize contradictions and inconsistencies in the generated text.',\n",
              " 'This significantly improves the quality of the generated text, and improves the user experience.',\n",
              " 'The Vertex Eval Service now scores LLM generated text and retrieved chunks on metrics like “coherence,” “fluency,” “groundedness,” \"safety,\" “instruction_following,” “question_answering_quality,” and more.',\n",
              " 'These metrics help you measure the grounded text you get from the LLM (for some metrics that is a comparison to a ground truth answer you have provided).',\n",
              " 'Implementing these evaluations gives you a baseline measurement and you can optimize for RAG quality by configuring your search engine, curating your source data, improving source layout parsing or chunking strategies, or refining the user’s question prior to search.',\n",
              " 'A RAG Ops, metrics driven approach like this will help you hill climb to high quality RAG and grounded generation.',\n",
              " 'RAGs, agents, and chatbots RAG and grounding can be integrated into any LLM application or agent which needs access to fresh, private, or specialized data.',\n",
              " 'By accessing external information, RAG-powered chatbots and conversational agents leverage external knowledge to provide more comprehensive, informative, and context-aware responses, improving the overall user experience.',\n",
              " 'Your data and your use case are what differentiate what you are building with gen AI.',\n",
              " 'RAG and grounding bring your data to LLMs efficiently and scalably.',\n",
              " 'What Google Cloud products and services are related to RAG?',\n",
              " 'The following Google Cloud products are related to Retrieval-Augmented Generation: Vertex AI SearchVertex AI Search is Google Search for your data, a fully managed, out-of-the-box search and RAG builder.',\n",
              " 'Vertex AI Vector SearchThe ultra performant vector index that powers Vertex AI Search; it enables semantic and hybrid search and retrieval from huge collections of embeddings with high recall at high query rate.',\n",
              " 'BigQueryLarge datasets that you can use to train machine learning models, including models for Vertex AI Vector Search.',\n",
              " 'Grounded Generation APIGemini high-fidelity mode grounded with Google Search or inline facts or bring your own search engine.',\n",
              " 'AlloyDBRun models in Vertex AI and access them in your application using familiar SQL queries.',\n",
              " 'Use Google models, such as Gemini, or your own custom models.',\n",
              " 'LlamaIndex on VertexBuild your own search engine for RAG and grounding using Google or open source components and our fully managed orchestration system based on LlamaIndex.',\n",
              " 'Learn more about using retrieval augmented generation with these resources.',\n",
              " 'Using Vertex AI to build next-gen search applications | Google Cloud Blog RAGs powered by Google Search technology RAG with databases on Google Cloud Infrastructure for a RAG-capable generative AI application using Vertex AI APIs to build your own search and Retrieval Augmented Generation (RAG) systems How to use RAG in BigQuery to bolster LLMs Code sample and quickstart to get familiar with RAG Start building on Google Cloud with $300 in free credits and 20+ always free products.',\n",
              " 'Need help getting started?Contact sales Work with a trusted partnerFind a partner Continue browsingSee all products What is Cloud Computing?',\n",
              " 'Cloud Computing Concepts Hub Artificial Intelligence Generative AI What is RAG (Retrieval-Augmented Generation)?',\n",
              " 'What is Retrieval-Augmented Generation?',\n",
              " 'Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response.',\n",
              " 'Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences.',\n",
              " \"RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model.\",\n",
              " 'It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.',\n",
              " 'Why is Retrieval-Augmented Generation important?',\n",
              " 'LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications.',\n",
              " 'The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources.',\n",
              " 'Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses.',\n",
              " 'Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.',\n",
              " 'Known challenges of LLMs include: Presenting false information when it does not have the answer.',\n",
              " 'Presenting out-of-date or generic information when the user expects a specific, current response.',\n",
              " 'Creating a response from non-authoritative sources.',\n",
              " 'Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.',\n",
              " 'You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence.',\n",
              " 'Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!',\n",
              " 'RAG is one approach to solving some of these challenges.',\n",
              " 'It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources.',\n",
              " 'Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.',\n",
              " 'What are the benefits of Retrieval-Augmented Generation?',\n",
              " \"RAG technology brings several benefits to an organization's generative AI efforts.\",\n",
              " 'Cost-effective implementation Chatbot development typically begins using a foundation model.',\n",
              " 'Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data.',\n",
              " 'The computational and financial costs of retraining FMs for organization or domain-specific information are high.',\n",
              " 'RAG is a more cost-effective approach to introducing new data to the LLM.',\n",
              " 'It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.',\n",
              " 'Current information Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy.',\n",
              " 'RAG allows developers to provide the latest research, statistics, or news to the generative models.',\n",
              " 'They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources.',\n",
              " 'The LLM can then provide the latest information to the users.',\n",
              " 'Enhanced user trust RAG allows the LLM to present accurate information with source attribution.',\n",
              " 'The output can include citations or references to sources.',\n",
              " 'Users can also look up source documents themselves if they require further clarification or more detail.',\n",
              " 'This can increase trust and confidence in your generative AI solution.',\n",
              " 'More developer control With RAG, developers can test and improve their chat applications more efficiently.',\n",
              " \"They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage.\",\n",
              " 'Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses.',\n",
              " 'In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions.',\n",
              " 'Organizations can implement generative AI technology more confidently for a broader range of applications.',\n",
              " 'How does Retrieval-Augmented Generation work?',\n",
              " 'Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows.',\n",
              " 'With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source.',\n",
              " 'The user query and the relevant information are both given to the LLM.',\n",
              " 'The LLM uses the new knowledge and its training data to create better responses.',\n",
              " 'The following sections provide an overview of the process.',\n",
              " \"Create external data The new data outside of the LLM's original training data set is called external data.\",\n",
              " 'It can come from multiple data sources, such as a APIs, databases, or document repositories.',\n",
              " 'The data may exist in various formats like files, database records, or long-form text.',\n",
              " 'Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database.',\n",
              " 'This process creates a knowledge library that the generative AI models can understand.',\n",
              " 'Retrieve relevant information The next step is to perform a relevancy search.',\n",
              " 'The user query is converted to a vector representation and matched with the vector databases.',\n",
              " 'For example, consider a smart chatbot that can answer human resource questions for an organization.',\n",
              " 'If an employee searches, \"How much annual leave do I have?\"',\n",
              " \"the system will retrieve annual leave policy documents alongside the individual employee's past leave record.\",\n",
              " 'These specific documents will be returned because they are highly-relevant to what the employee has input.',\n",
              " 'The relevancy was calculated and established using mathematical vector calculations and representations.',\n",
              " 'Augment the LLM prompt Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context.',\n",
              " 'This step uses prompt engineering techniques to communicate effectively with the LLM.',\n",
              " 'The augmented prompt allows the large language models to generate an accurate answer to user queries.',\n",
              " 'Update external data The next question may be—what if the external data becomes stale?',\n",
              " 'To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents.',\n",
              " 'You can do this through automated real-time processes or periodic batch processing.',\n",
              " 'This is a common challenge in data analytics—different data-science approaches to change management can be used.',\n",
              " 'The following diagram shows the conceptual flow of using RAG with LLMs.',\n",
              " 'What is the difference between Retrieval-Augmented Generation and semantic search?',\n",
              " 'Semantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications.',\n",
              " 'Modern enterprises store vast amounts of information like manuals, FAQs, research reports, customer service guides, and human resource document repositories across various systems.',\n",
              " 'Context retrieval is challenging at scale and consequently lowers generative output quality.',\n",
              " 'Semantic search technologies can scan large databases of disparate information and retrieve data more accurately.',\n",
              " 'For example, they can answer questions such as, \"How much was spent on machinery repairs last year?” by mapping the question to the relevant documents and returning specific text instead of search results.',\n",
              " 'Developers can then use that answer to provide more context to the LLM.',\n",
              " 'Conventional or keyword search solutions in RAG produce limited results for knowledge-intensive tasks.',\n",
              " 'Developers must also deal with word embeddings, document chunking, and other complexities as they manually prepare their data.',\n",
              " \"In contrast, semantic search technologies do all the work of knowledge base preparation so developers don't have to.\",\n",
              " 'They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload.',\n",
              " 'How can AWS support your Retrieval-Augmented Generation requirements?',\n",
              " 'Amazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models—along with a broad set of capabilities—to build generative AI applications while simplifying development and maintaining privacy and security.',\n",
              " 'With knowledge bases for Amazon Bedrock, you can connect FMs to your data sources for RAG in just a few clicks.',\n",
              " 'Vector conversions, retrievals, and improved output generation are all handled automatically.',\n",
              " 'For organizations managing their own RAG, Amazon Kendra is a highly-accurate enterprise search service powered by machine learning.',\n",
              " 'It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra’s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows.',\n",
              " 'For example, with the Retrieve API, you can: Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.',\n",
              " 'Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.',\n",
              " 'Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.',\n",
              " 'Filter responses based on those documents that the end-user permissions allow.',\n",
              " 'Amazon also offers options for organizations who want to build more custom generative AI solutions.',\n",
              " 'Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks.',\n",
              " 'You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.',\n",
              " 'Get started with Retrieval-Augmented Generation on AWS by creating a free account today Next Steps on AWS Instant get access to the AWS Free Tier.',\n",
              " 'Get started building in the AWS management console.',\n",
              " 'Accessibility Policy Skip to content QUICK LINKS Oracle Cloud Infrastructure Oracle Fusion Cloud Applications Oracle Database Download Java Careers at Oracle What Is Retrieval-Augmented Generation (RAG)?',\n",
              " 'Alan Zeichick | Tech Content Strategist | September 19, 2023 In This Article What Is Retrieval-Augmented Generation (RAG)?',\n",
              " 'Retrieval-Augmented Generation Explained How Does Retrieval-Augmented Generation Work?',\n",
              " 'Using RAG in Chat Applications Benefits of Retrieval-Augmented Generation Challenges of Retrieval-Augmented Generation Examples of Retrieval-Augmented Generation Future of Retrieval-Augmented Generation Generative AI With Oracle Retrieval-Augmented Generation FAQs Generative artificial intelligence (AI) excels at creating text responses based on large language models (LLMs) where the AI is trained on a massive number of data points.',\n",
              " 'The good news is that the generated text is often easy to read and provides detailed responses that are broadly applicable to the questions asked of the software, often called prompts.',\n",
              " 'The bad news is that the information used to generate the response is limited to the information used to train the AI, often a generalized LLM.',\n",
              " 'The LLM’s data may be weeks, months, or years out of date and in a corporate AI chatbot may not include specific information about the organization’s products or services.',\n",
              " 'That can lead to incorrect responses that erode confidence in the technology among customers and employees.',\n",
              " 'What Is Retrieval-Augmented Generation (RAG)?',\n",
              " 'That’s where retrieval-augmented generation (RAG) comes in.',\n",
              " 'RAG provides a way to optimize the output of an LLM with targeted information without modifying the underlying model itself; that targeted information can be more up-to-date than the LLM as well as specific to a particular organization and industry.',\n",
              " 'That means the generative AI system can provide more contextually appropriate answers to prompts as well as base those answers on extremely current data.',\n",
              " 'RAG first came to the attention of generative AI developers after the publication of “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” a 2020 paper published by Patrick Lewis and a team at Facebook AI Research.',\n",
              " 'The RAG concept has been embraced by many academic and industry researchers, who see it as a way to significantly improve the value of generative AI systems.',\n",
              " 'Retrieval-Augmented Generation Explained Consider a sports league that wants fans and the media to be able to use chat to access its data and answer questions about players, teams, the sport’s history and rules, and current stats and standings.',\n",
              " 'A generalized LLM could answer questions about the history and rules or perhaps describe a particular team’s stadium.',\n",
              " 'It wouldn’t be able to discuss last night’s game or provide current information about a particular athlete’s injury because the LLM wouldn’t have that information—and given that an LLM takes significant computing horsepower to retrain, it isn’t feasible to keep the model current.',\n",
              " 'In addition to the large, fairly static LLM, the sports league owns or can access many other information sources, including databases, data warehouses, documents containing player bios, and news feeds that discuss each game in depth.',\n",
              " 'RAG lets the generative AI ingest this information.',\n",
              " 'Now, the chat can provide information that’s more timely, more contextually appropriate, and more accurate.',\n",
              " 'Simply put, RAG helps LLMs give better answers.',\n",
              " 'Key Takeaways RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining.',\n",
              " 'RAG models build knowledge repositories based on the organization’s own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers.',\n",
              " 'Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI.',\n",
              " 'Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM.',\n",
              " 'How Does Retrieval-Augmented Generation Work?',\n",
              " 'Consider all the information that an organization has—the structured databases, the unstructured PDFs and other documents, the blogs, the news feeds, the chat transcripts from past customer service sessions.',\n",
              " 'In RAG, this vast quantity of dynamic data is translated into a common format and stored in a knowledge library that’s accessible to the generative AI system.',\n",
              " 'The data in that knowledge library is then processed into numerical representations using a special type of algorithm called an embedded language model and stored in a vector database, which can be quickly searched and used to retrieve the correct contextual information.',\n",
              " 'RAG and Large Language Models (LLMs) Now, say an end user sends the generative AI system a specific prompt, for example, “Where will tonight’s game be played, who are the starting players, and what are reporters saying about the matchup?” The query is transformed into a vector and used to query the vector database, which retrieves information relevant to that question’s context.',\n",
              " 'That contextual information plus the original prompt are then fed into the LLM, which generates a text response based on both its somewhat out-of-date generalized knowledge and the extremely timely contextual information.',\n",
              " 'Interestingly, while the process of training the generalized LLM is time-consuming and costly, updates to the RAG model are just the opposite.',\n",
              " 'New data can be loaded into the embedded language model and translated into vectors on a continuous, incremental basis.',\n",
              " 'In fact, the answers from the entire generative AI system can be fed back into the RAG model, improving its performance and accuracy, because, in effect, it knows how it has already answered a similar question.',\n",
              " 'An additional benefit of RAG is that by using the vector database, the generative AI can provide the specific source of data cited in its answer—something LLMs can’t do.',\n",
              " 'Therefore, if there’s an inaccuracy in the generative AI’s output, the document that contains that erroneous information can be quickly identified and corrected, and then the corrected information can be fed into the vector database.',\n",
              " 'In short, RAG provides timeliness, context, and accuracy grounded in evidence to generative AI, going beyond what the LLM itself can provide.',\n",
              " 'Retrieval-Augmented Generation vs. Semantic Search RAG isn’t the only technique used to improve the accuracy of LLM-based generative AI.',\n",
              " 'Another technique is semantic search, which helps the AI system narrow down the meaning of a query by seeking deep understanding of the specific words and phrases in the prompt.',\n",
              " 'Traditional search is focused on keywords.',\n",
              " 'For example, a basic query asking about the tree species native to France might search the AI system’s database using “trees” and “France” as keywords and find data that contains both keywords—but the system might not truly comprehend the meaning of trees in France and therefore may retrieve too much information, too little, or even the wrong information.',\n",
              " 'That keyword-based search might also miss information because the keyword search is too literal: The trees native to Normandy might be missed, even though they’re in France, because that keyword was missing.',\n",
              " 'Semantic search goes beyond keyword search by determining the meaning of questions and source documents and using that meaning to retrieve more accurate results.',\n",
              " 'Semantic search is an integral part of RAG.',\n",
              " 'Using RAG in Chat Applications When a person wants an instant answer to a question, it’s hard to beat the immediacy and usability of a chatbot.',\n",
              " 'Most bots are trained on a finite number of intents—that is, the customer’s desired tasks or outcomes—and they respond to those intents.',\n",
              " 'RAG capabilities can make current bots better by allowing the AI system to provide natural language answers to questions that aren’t in the intent list.',\n",
              " 'The “ask a question, get an answer” paradigm makes chatbots a perfect use case for generative AI, for many reasons.',\n",
              " 'Questions often require specific context to generate an accurate answer, and given that chatbot users’ expectations about relevance and accuracy are often high, it’s clear how RAG techniques apply.',\n",
              " 'In fact, for many organizations, chatbots may indeed be the starting point for RAG and generative AI use.',\n",
              " 'Questions often require specific context to deliver an accurate answer.',\n",
              " 'Customer queries about a newly introduced product, for example, aren’t useful if the data pertains to the previous model and may in fact be misleading.',\n",
              " 'And a hiker who wants to know if a park is open this Sunday expects timely, accurate information about that specific park on that specific date.',\n",
              " 'Benefits of Retrieval-Augmented Generation RAG techniques can be used to improve the quality of a generative AI system’s responses to prompts, beyond what an LLM alone can deliver.',\n",
              " 'Benefits include the following: The RAG has access to information that may be fresher than the data used to train the LLM.',\n",
              " 'Data in the RAG’s knowledge repository can be continually updated without incurring significant costs.',\n",
              " 'The RAG’s knowledge repository can contain data that’s more contextual than the data in a generalized LLM.',\n",
              " 'The source of the information in the RAG’s vector database can be identified.',\n",
              " 'And because the data sources are known, incorrect information in the RAG can be corrected or deleted.',\n",
              " 'Challenges of Retrieval-Augmented Generation Because RAG is a relatively new technology, first proposed in 2020, AI developers are still learning how to best implement its information retrieval mechanisms in generative AI.',\n",
              " 'Some key challenges are Improving organizational knowledge and understanding of RAG because it’s so new Increasing costs; while generative AI with RAG will be more expensive to implement than an LLM on its own, this route is less costly than frequently retraining the LLM itself Determining how to best model the structured and unstructured data within the knowledge library and vector database Developing requirements for a process to incrementally feed data into the RAG system Putting processes in place to handle reports of inaccuracies and to correct or delete those information sources in the RAG system Examples of Retrieval-Augmented Generation There are many possible examples of generative AI augmented by RAG.',\n",
              " 'Cohere, a leader in the field of generative AI and RAG, has written about a chatbot that can provide contextual information about a vacation rental in the Canary Islands, including fact-based answers about beach accessibility, lifeguards on nearby beaches, and the availability of volleyball courts within walking distance.',\n",
              " 'Oracle has described other use cases for RAG, such as analyzing financial reports, assisting with gas and oil discovery, reviewing transcripts from call center customer exchanges, and searching medical databases for relevant research papers.',\n",
              " 'Future of Retrieval-Augmented Generation Today, in the early phases of RAG, the technology is being used to provide timely, accurate, and contextual responses to queries.',\n",
              " 'These use cases are appropriate to chatbots, email, text messaging, and other conversational applications.',\n",
              " 'In the future, possible directions for RAG technology would be to help generative AI take an appropriate action based on contextual information and user prompts.',\n",
              " 'For example, a RAG-augmented AI system might identify the highest-rated beach vacation rental on the Canary Islands and then initiate booking a two-bedroom cabin within walking distance of the beach during a volleyball tournament.',\n",
              " 'RAG might also be able to assist with more sophisticated lines of questioning.',\n",
              " 'Today, generative AI might be able to tell an employee about the company’s tuition reimbursement policy; RAG could add more contextual data to tell the employee which nearby schools have courses that fit into that policy and perhaps recommend programs that are suited to the employee’s jobs and previous training—maybe even help apply for those programs and initiate a reimbursement request.',\n",
              " 'Generative AI With Oracle Oracle offers a variety of advanced cloud-based AI services, including the OCI Generative AI service running on Oracle Cloud Infrastructure (OCI).',\n",
              " 'Oracle’s offerings include robust models based on your organization’s unique data and industry knowledge.',\n",
              " 'Customer data is not shared with LLM providers or seen by other customers, and custom models trained on customer data can only be used by that customer.',\n",
              " 'In addition, Oracle is integrating generative AI across its wide range of cloud applications, and generative AI capabilities are available to developers who use OCI and across its database portfolio.',\n",
              " 'What’s more, Oracle’s AI services offer predictable performance and pricing using single-tenant AI clusters dedicated to your use.',\n",
              " 'The power and capabilities of LLMs and generative AI are widely known and understood—they’ve been the subject of breathless news headlines for the past year.',\n",
              " 'Retrieval-augmented generation builds on the benefits of LLMs by making them more timely, more accurate, and more contextual.',\n",
              " 'For business applications of generative AI, RAG is an important technology to watch, study, and pilot.',\n",
              " 'Oracle offers a modern data platform and low-cost, high-performance AI infrastructure.',\n",
              " 'Additional factors, such as powerful, high-performing models, unrivaled data security, and embedded AI services demonstrate why Oracle’s AI offering is truly built for enterprises.',\n",
              " 'Retrieval-Augmented Generation FAQs Is RAG the same as generative AI?',\n",
              " 'No.',\n",
              " 'Retrieval-augmented generation is a technique that can provide more accurate results to queries than a generative large language model on its own because RAG uses knowledge external to data already contained in the LLM.',\n",
              " 'What type of information is used in RAG?',\n",
              " 'RAG can incorporate data from many sources, such as relational databases, unstructured document repositories, internet data streams, media newsfeeds, audio transcripts, and transaction logs.',\n",
              " 'How does generative AI use RAG?',\n",
              " 'Data from enterprise data sources is embedded into a knowledge repository and then converted to vectors, which are stored in a vector database.',\n",
              " 'When an end user makes a query, the vector database retrieves relevant contextual information.',\n",
              " 'This contextual information, along with the query, is sent to the large language model, which uses the context to create a more timely, accurate, and contextual response.',\n",
              " 'Can a RAG cite references for the data it retrieves?',\n",
              " 'Yes.',\n",
              " 'The vector databases and knowledge repositories used by RAG contain specific information about the sources of information.',\n",
              " 'This means that sources can be cited, and if there’s an error in one of those sources it can be quickly corrected or deleted so that subsequent queries won’t return that incorrect information.']"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(sentences, convert_to_numpy = True)"
      ],
      "metadata": {
        "id": "s7lCVbO6bZrc"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z_Wxcb3p_DM",
        "outputId": "9933876a-e35e-4faa-e1e6-9b729da2b0fb"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(232, 384)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "WvX5zTQDhmco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6a4983-cf47-4f1b-d99e-a172be633bbe"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.12751824,  0.01289594, -0.01769432, ...,  0.02670551,\n",
              "        -0.02776622,  0.02087011],\n",
              "       [-0.06509098, -0.01325854,  0.02227048, ...,  0.03335963,\n",
              "         0.01122819,  0.02471756],\n",
              "       [ 0.04819336, -0.10355049,  0.00281728, ..., -0.00797717,\n",
              "        -0.05605378, -0.00971238],\n",
              "       ...,\n",
              "       [-0.12919769,  0.0351914 ,  0.01423578, ..., -0.03317212,\n",
              "         0.07071075, -0.0186689 ],\n",
              "       [ 0.01188224,  0.0346556 , -0.04887683, ...,  0.00467257,\n",
              "         0.04265281,  0.03269922],\n",
              "       [-0.06443138, -0.00091817, -0.05409248, ...,  0.04765166,\n",
              "         0.06305917,  0.02972038]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def elbow_method(embeddings, max_k=25):\n",
        "    distortions = []\n",
        "    for k in range(1, max_k + 1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        kmeans.fit(embeddings)\n",
        "        distortions.append(kmeans.inertia_)\n",
        "    return distortions\n",
        "\n",
        "max_k = 15\n",
        "\n",
        "distortions = elbow_method(embeddings, max_k=max_k)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, max_k + 1), distortions, marker='o', color='b')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Distortion (Inertia)')\n",
        "plt.xticks(range(1, max_k + 1))\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "fWbampObikPI",
        "outputId": "beb82c94-db98-4ca0-8f1b-4cdacd4a8d0f"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def silhouette_scores(embeddings, max_k=10):\n",
        "    scores = []\n",
        "    for k in range(2, max_k + 1):  # Silhouette score requires at least 2 clusters\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(embeddings)\n",
        "        score = silhouette_score(embeddings, cluster_labels)\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "# Calculate silhouette scores\n",
        "max_k = 15\n",
        "silhouette_scores_list = silhouette_scores(embeddings, max_k=max_k)\n",
        "\n",
        "# Plot the silhouette scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(2, max_k + 1), silhouette_scores_list, marker='o', color='g')\n",
        "plt.title('Silhouette Scores for Optimal k')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.xticks(range(2, max_k + 1))\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Q7gE4a37jSRN",
        "outputId": "c2a2dc77-27e4-4ba1-b845-96fcba388fc9"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 10\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(embeddings)\n",
        "\n",
        "# Get cluster labels\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Separate embeddings by clusters\n",
        "chunks = {i: [] for i in range(n_clusters)}\n",
        "for idx, label in enumerate(cluster_labels):\n",
        "    chunks[label].append(embeddings[idx])\n",
        "\n",
        "# Convert chunks to numpy arrays\n",
        "chunks = {key: np.array(value) for key, value in chunks.items()}"
      ],
      "metadata": {
        "id": "CnTQ_ZVpkA0G"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9ZVn3k5dlmBc",
        "outputId": "7abcc313-2bb6-49ef-acba-d3faf2e08b05"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: array([[ 0.00653037,  0.02076948, -0.00057105, ...,  0.09527557,\n",
              "         -0.03733634,  0.01172701],\n",
              "        [ 0.06162129, -0.05024869,  0.02451289, ...,  0.07237758,\n",
              "         -0.10747138,  0.01706683],\n",
              "        [ 0.04424914, -0.04170283,  0.01515204, ...,  0.01166166,\n",
              "         -0.0299067 , -0.0134074 ],\n",
              "        ...,\n",
              "        [ 0.03797891, -0.02593031, -0.06811847, ...,  0.02276054,\n",
              "         -0.02315169,  0.04700333],\n",
              "        [ 0.01236148,  0.01259603, -0.00438966, ...,  0.11714011,\n",
              "         -0.0023437 , -0.00465269],\n",
              "        [-0.00024769, -0.08716257,  0.01316435, ..., -0.0325728 ,\n",
              "         -0.05612245, -0.04530084]], dtype=float32),\n",
              " 1: array([[-0.03932015, -0.03438421,  0.00565081, ...,  0.00046228,\n",
              "          0.01067668,  0.03162077],\n",
              "        [-0.05326812, -0.01492053, -0.04931074, ...,  0.00658254,\n",
              "          0.09148338,  0.09207619],\n",
              "        [-0.08577175, -0.02280188, -0.05356457, ...,  0.01025227,\n",
              "          0.02800286,  0.07306206],\n",
              "        [-0.02719283, -0.07455506, -0.07568567, ..., -0.06896795,\n",
              "          0.11977713,  0.07988767],\n",
              "        [-0.0320617 , -0.04671163, -0.00302315, ..., -0.04660657,\n",
              "          0.11772066,  0.0212474 ]], dtype=float32),\n",
              " 2: array([[ 0.04819336, -0.10355049,  0.00281728, ..., -0.00797717,\n",
              "         -0.05605378, -0.00971238],\n",
              "        [ 0.01774056, -0.05797415,  0.00873997, ...,  0.02507077,\n",
              "         -0.03084807, -0.00626341],\n",
              "        [-0.07198822,  0.00757942, -0.04034671, ...,  0.03503025,\n",
              "         -0.04333111,  0.05211185],\n",
              "        ...,\n",
              "        [-0.03348152, -0.04836933, -0.06440675, ...,  0.05193   ,\n",
              "          0.06852564,  0.02543941],\n",
              "        [-0.04206217, -0.10237002, -0.1020935 , ..., -0.02308551,\n",
              "          0.05674484,  0.07164508],\n",
              "        [ 0.06001743, -0.04197674, -0.06607819, ..., -0.01436002,\n",
              "         -0.05594699,  0.02373412]], dtype=float32),\n",
              " 3: array([[-0.10620873,  0.01525934,  0.09688609, ...,  0.05477285,\n",
              "          0.0595089 ,  0.00118801],\n",
              "        [-0.08149704, -0.01033082, -0.00079321, ...,  0.04501809,\n",
              "          0.06883191,  0.00998435],\n",
              "        [-0.08591741,  0.03576851,  0.11143892, ..., -0.01489994,\n",
              "          0.0502969 ,  0.00781418],\n",
              "        ...,\n",
              "        [-0.08568778,  0.02742143,  0.04014405, ...,  0.01929605,\n",
              "          0.03754498,  0.00588308],\n",
              "        [-0.11378706, -0.04269732, -0.0062581 , ...,  0.07827858,\n",
              "          0.02527983,  0.01779607],\n",
              "        [-0.09315269,  0.02522219,  0.08941144, ...,  0.05449105,\n",
              "          0.05863739, -0.02848653]], dtype=float32),\n",
              " 4: array([[-0.12751824,  0.01289594, -0.01769432, ...,  0.02670551,\n",
              "         -0.02776622,  0.02087011],\n",
              "        [-0.08212694, -0.01003115, -0.04537413, ...,  0.04338799,\n",
              "         -0.06561147,  0.0387565 ],\n",
              "        [-0.02823809, -0.03204469, -0.02147207, ...,  0.10841992,\n",
              "         -0.08977151,  0.0655754 ],\n",
              "        ...,\n",
              "        [-0.10460493,  0.03195389, -0.02259539, ...,  0.01603882,\n",
              "         -0.04590288,  0.06206176],\n",
              "        [-0.02966872, -0.01872402,  0.00803612, ...,  0.07471532,\n",
              "         -0.10576744,  0.01036343],\n",
              "        [-0.0764823 , -0.03258596,  0.01050573, ...,  0.03648713,\n",
              "         -0.01014771,  0.06558827]], dtype=float32),\n",
              " 5: array([[ 0.00036304, -0.10755386, -0.01018996, ...,  0.04243759,\n",
              "         -0.00286475, -0.03771486],\n",
              "        [-0.03666703,  0.07552186,  0.02188777, ...,  0.03441697,\n",
              "         -0.01179641,  0.08158936],\n",
              "        [-0.08902881,  0.01231612, -0.07319301, ..., -0.002398  ,\n",
              "          0.04628156,  0.07616836],\n",
              "        ...,\n",
              "        [-0.02572288, -0.05947311,  0.0598219 , ..., -0.07027971,\n",
              "          0.05440703,  0.08612164],\n",
              "        [ 0.02546519, -0.04527044,  0.07530564, ..., -0.01393182,\n",
              "         -0.08671271, -0.03763727],\n",
              "        [-0.06443138, -0.00091817, -0.05409248, ...,  0.04765166,\n",
              "          0.06305917,  0.02972038]], dtype=float32),\n",
              " 6: array([[-0.01710627,  0.01131468,  0.0179257 , ..., -0.0818496 ,\n",
              "          0.00883133,  0.01818926],\n",
              "        [ 0.03679276,  0.03210606, -0.0389211 , ...,  0.05366338,\n",
              "         -0.07221066,  0.00627225],\n",
              "        [-0.05709515,  0.02878293,  0.00903992, ...,  0.0955562 ,\n",
              "          0.06989834,  0.00901527],\n",
              "        ...,\n",
              "        [-0.09055622, -0.03433973,  0.01877088, ...,  0.1199987 ,\n",
              "          0.04837058,  0.02759619],\n",
              "        [-0.0726951 ,  0.01450976,  0.0067802 , ..., -0.01927622,\n",
              "          0.0276712 ,  0.01633652],\n",
              "        [-0.12919769,  0.0351914 ,  0.01423578, ..., -0.03317212,\n",
              "          0.07071075, -0.0186689 ]], dtype=float32),\n",
              " 7: array([[ 0.00771281,  0.05016687,  0.02064577, ...,  0.04467225,\n",
              "          0.09058651,  0.00034021],\n",
              "        [-0.00712475,  0.02586659,  0.02169967, ..., -0.02169944,\n",
              "          0.03505949,  0.03176929],\n",
              "        [-0.01159802, -0.03294065, -0.03458632, ...,  0.02052106,\n",
              "          0.02409759,  0.02489914],\n",
              "        ...,\n",
              "        [ 0.03899037, -0.0039452 , -0.07816701, ..., -0.01640704,\n",
              "          0.0714689 , -0.0223371 ],\n",
              "        [ 0.05828492,  0.02876257, -0.08245055, ...,  0.06007478,\n",
              "         -0.00365724, -0.00125477],\n",
              "        [ 0.0231906 ,  0.03506126, -0.00539137, ...,  0.13584289,\n",
              "          0.00078337,  0.02015729]], dtype=float32),\n",
              " 8: array([[-0.01591204, -0.03556608, -0.0044142 , ...,  0.0043486 ,\n",
              "          0.00982224,  0.03204119],\n",
              "        [-0.06279726, -0.01043529, -0.0044497 , ...,  0.02420804,\n",
              "         -0.0020871 , -0.03665189],\n",
              "        [-0.14279558, -0.03002413,  0.0102784 , ...,  0.01749731,\n",
              "          0.00820221,  0.00977257],\n",
              "        ...,\n",
              "        [-0.08279502, -0.06429636,  0.00983142, ...,  0.01204094,\n",
              "          0.11147632, -0.00675939],\n",
              "        [-0.00371998, -0.08658262, -0.02515114, ..., -0.03435485,\n",
              "          0.03770696, -0.01742926],\n",
              "        [-0.00979206, -0.01966183, -0.0049997 , ..., -0.0281986 ,\n",
              "          0.07380661,  0.00131718]], dtype=float32),\n",
              " 9: array([[-0.06509098, -0.01325854,  0.02227048, ...,  0.03335963,\n",
              "          0.01122819,  0.02471756],\n",
              "        [-0.08209604,  0.09216508,  0.05418096, ..., -0.01977584,\n",
              "          0.03868715,  0.03581056],\n",
              "        [-0.05049631,  0.07401263,  0.03981725, ...,  0.06617358,\n",
              "          0.03557945, -0.00332012],\n",
              "        ...,\n",
              "        [-0.03887677,  0.02150311, -0.03622337, ..., -0.02139371,\n",
              "          0.06661193,  0.02129005],\n",
              "        [-0.11377483,  0.07481176, -0.05946325, ..., -0.0241952 ,\n",
              "          0.05596616,  0.02311588],\n",
              "        [ 0.01188224,  0.0346556 , -0.04887683, ...,  0.00467257,\n",
              "          0.04265281,  0.03269922]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "W7jU9Q-uHH5K"
      },
      "outputs": [],
      "source": [
        "faiss_indices = {}\n",
        "\n",
        "for cluster_id, cluster_embeddings in chunks.items():\n",
        "    dimension = cluster_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "    index.add(cluster_embeddings)\n",
        "\n",
        "    faiss_indices[cluster_id] = index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-VBQFpVnCCe",
        "outputId": "238ae766-0993-42c1-9f07-de3391680433"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: <faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7be735db7c30> >,\n",
              " 1: <faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7be5cdbad1a0> >,\n",
              " 2: <faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7be5cdbac570> >,\n",
              " 3: <faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7be5cdcda610> >,\n",
              " 4: <faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7be5cdcd9860> >,\n",
              " 5: <faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7be5cdcda520> >,\n",
              " 6: <faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7be5cdcd93e0> >,\n",
              " 7: <faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7be5cd5ce7c0> >,\n",
              " 8: <faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7be5cd5cf180> >,\n",
              " 9: <faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7be5cd5ccfc0> >}"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sToXoJscHH5M"
      },
      "source": [
        "# RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV0P1QUqHH5N"
      },
      "source": [
        "## Retrieval (3 nearest clusters using similarity search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "hWsGXbBbHH5N"
      },
      "outputs": [],
      "source": [
        "query_sentence = query\n",
        "query_embedding = model.encode([query_sentence], convert_to_numpy=True)\n",
        "\n",
        "predicted_cluster = kmeans.predict(query_embedding)[0]\n",
        "relevant_index = faiss_indices[predicted_cluster]\n",
        "\n",
        "D, I = relevant_index.search(query_embedding, k=5)  # Top 5 nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S5-0rppngeQ",
        "outputId": "f32dbe84-ea84-4f13-c688-ac121cf8b8d0"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.56204164, 0.7153136 , 0.7776549 , 0.7838857 , 0.83863693]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "I"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tWF5kAunjyt",
        "outputId": "ccc46aff-5377-4631-8575-524d1aef52b7"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1, 19, 40, 25,  5]])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_passages = [sentences[index] for index in I[0]]"
      ],
      "metadata": {
        "id": "AINR5XaHn376"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_passages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUZ3OU1Sn7sH",
        "outputId": "42c0b0bb-589b-456e-e631-1e90607d83ac"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs).',\n",
              " 'Search with vector databases and relevancy re-rankers RAGs usually retrieve facts via search, and modern search engines now leverage vector databases to efficiently retrieve relevant documents.',\n",
              " 'BigQueryLarge datasets that you can use to train machine learning models, including models for Vertex AI Vector Search.',\n",
              " 'You need the best semantic search on top of a curated knowledge base to ensure that the retrieved information is relevant to the input query or context.',\n",
              " 'Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words.']"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "pXD7oKSXHH5N"
      },
      "outputs": [],
      "source": [
        "relevant_passages_str = ''.join(relevant_passages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "uT4y60wcHH5O",
        "outputId": "9b3b8f94-3cd7-42dd-9a8f-d0d08c76adc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs).Search with vector databases and relevancy re-rankers RAGs usually retrieve facts via search, and modern search engines now leverage vector databases to efficiently retrieve relevant documents.BigQueryLarge datasets that you can use to train machine learning models, including models for Vertex AI Vector Search.You need the best semantic search on top of a curated knowledge base to ensure that the retrieved information is relevant to the input query or context.Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 147
        }
      ],
      "source": [
        "relevant_passages_str"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(relevant_passages_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRwcS9TfjVq_",
        "outputId": "6a0aff6e-51a4-4b05-b25c-741d63049801"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "816"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzNkN_IZHH5O"
      },
      "source": [
        "## Augmented generation (with RAG)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is \" + query\n",
        "query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q_89Hh4gu3xA",
        "outputId": "bf14cc08-82cc-4703-a8a7-c0348946c096"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is RAG system?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "QP4KGAXVHH5O"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": relevant_passages_str},\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwAFtbbfHH5O",
        "outputId": "60d570cb-d4ab-4c22-aa41-c5588875f64b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What is RAG system?\n",
            "Response:  RAG (Relevant Agent) is an AI-powered conversational assistant that can help users with various tasks, including answering questions, providing information, and even assisting in generating text. It works by analyzing large amounts of data to understand patterns and relationships between users' inputs, allowing it to provide accurate and relevant responses.\n",
            "Sources:\n",
            "https://cloud.google.com/use-cases/retrieval-augmented-generation\n",
            "https://aws.amazon.com/what-is/retrieval-augmented-generation/\n",
            "https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/\n"
          ]
        }
      ],
      "source": [
        "print(\"Question: \", query)\n",
        "print(\"Response: \", response)\n",
        "print(\"Sources:\")\n",
        "for url in final_urls:\n",
        "  print(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G1E6r3mMUpf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Control data (same prompt without RAG)"
      ],
      "metadata": {
        "id": "8KwDxVi_0jIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "amL5fgNv0P-a"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Question: \", query)\n",
        "print(\"Response: \", response)"
      ],
      "metadata": {
        "id": "nzO0vMZY0TDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a23db5-5b96-4882-c211-628cd88934d1"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What is RAG system?\n",
            "Response:  RAG (Relevant Agent) is an AI-powered conversational assistant that can help users with various tasks, including answering questions, providing information, and even assisting in generating text. It works by analyzing large amounts of data to understand patterns and relationships between users' inputs, allowing it to provide accurate and relevant responses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MC1qgr5D0pKZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "RV0P1QUqHH5N"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}