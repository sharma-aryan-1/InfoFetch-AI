{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2k4wwjxHLjH",
        "outputId": "d5ee6e8f-a6bc-4ba3-d9e1-08a4afe4a284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 21 05:14:35 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x5nFSN2HH5F"
      },
      "source": [
        "# Get content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz1f7LPiHads",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d4f85b-badc-430e-fb2b-4091026a19e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu -q\n",
        "!pip install beautifulsoup4 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B7GagRZHH5H"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "query = \"What is a RAG system?\"\n",
        "query = query.replace(' ', '+')\n",
        "\n",
        "url = f\"https://www.google.com/search?q={query}\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    urls = []\n",
        "    gfg = 0\n",
        "    for g in soup.find_all('a'):\n",
        "        href = g.get('href')\n",
        "        if href and \"/url?esrc=s&q=&rct=j&sa=U&url=\" in href:\n",
        "            link = href.split(\"/url?esrc=s&q=&rct=j&sa=U&url=\")[1].split(\"&\")[0]\n",
        "            #print(link)\n",
        "            #if (link.split(\"https://\")[1].split(\".\")[0] != \"scholar\"):\n",
        "            urls.append(link)\n",
        "else:\n",
        "    print(\"Failed to fetch results\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsEyhNksCOOO",
        "outputId": "30eb0b9b-4039-4985-9640-2813a78949a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUu0I5HcVET3",
        "outputId": "c12fab11-b543-463f-9736-f27f13263896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.cohesity.com/glossary/retrieval-augmented-generation-rag/',\n",
              " 'https://aws.amazon.com/what-is/retrieval-augmented-generation/',\n",
              " 'https://cloud.google.com/use-cases/retrieval-augmented-generation',\n",
              " 'https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/',\n",
              " 'https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/',\n",
              " 'https://www.oracle.com/in/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/',\n",
              " 'https://www.oracle.com/uk/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/',\n",
              " 'https://www.oracle.com/au/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/',\n",
              " 'https://www.oracle.com/ie/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/',\n",
              " 'https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag',\n",
              " 'https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag%23what-is-rag%3F-rag,o',\n",
              " 'https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag%23why-use-rag-to-improve-llms%3F-an-example-tobet',\n",
              " 'https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag%23personalized-recommendations-ragsy',\n",
              " 'https://www.databricks.com/glossary/retrieval-augmented-generation-rag',\n",
              " 'https://medium.com/%40sahin.samia/what-is-retrieval-augmented-generation-rag-in-llm-and-how-it-works-a8c79e35a172',\n",
              " 'https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/',\n",
              " 'https://research.ibm.com/blog/retrieval-augmented-generation-RAG']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "final_urls = []\n",
        "total = 0\n",
        "for url in urls:\n",
        "    success = False\n",
        "    while not success:\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                for tag in soup(['script', 'style', 'header', 'footer', 'nav']):\n",
        "                    tag.decompose()\n",
        "\n",
        "                page_text = ' '.join(tag.get_text() for tag in soup.find_all(['p', 'h1', 'h2', 'h3', 'li']))\n",
        "                texts.append(page_text)\n",
        "                success = True\n",
        "                total += 1\n",
        "                final_urls.append(url)\n",
        "            else:\n",
        "                print(f\"Failed to fetch {url}: {response.status_code}\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "            break\n",
        "\n",
        "    if (total == 3):\n",
        "        break\n"
      ],
      "metadata": {
        "id": "o1--aV5iIzcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_urls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkzNLQf8xc8J",
        "outputId": "040dbc29-6fdf-4301-ac1d-3a2d13148826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.cohesity.com/glossary/retrieval-augmented-generation-rag/',\n",
              " 'https://aws.amazon.com/what-is/retrieval-augmented-generation/',\n",
              " 'https://cloud.google.com/use-cases/retrieval-augmented-generation']"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1l6QKFZRweK",
        "outputId": "d217915d-7eb3-4a38-ef50-693e1780653f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Cohesity named a Leader again! 2023 Gartner® Magic Quadrant™ for Enterprise Backup and Recovery Software Solutions Glossary Retrieval augmented generation (RAG) Retrieval augmented generation (RAG) What is retrieval augmented generation (RAG)? Retrieval augmented generation (RAG) is a natural language processing (NLP) technique that combines the strengths of both retrieval- and generative-based artificial intelligence (AI) models. RAG AI can deliver accurate results that make the most of pre-existing knowledge but can also process and consolidate that knowledge to create unique, context-aware answers, instructions, or explanations in human-like language rather than just summarizing the retrieved data. RAG AI differs from generative AI in that it is a superset of generative AI. RAG combines the strengths of both generative AI and retrieval AI. RAG is also different from cognitive AI, which mimics the way the human brain works to get its results. How does retrieval augmented generation (RAG) work? RAG, short for retrieval augmented generation, works by integrating retrieval-based techniques with generative-based AI models. Retrieval-based models excel at extracting information from pre-existing online sources like newspaper articles, databases, blogs, and other knowledge repositories such as Wikipedia or even internal databases. However, such models cannot produce original or unique responses. Alternatively, generative models can generate original responses that are appropriate within the context of what is being asked, but can find it difficult to maintain strict accuracy. RAG was developed to overcome these relative weaknesses in existing models to combine their strengths and minimize drawbacks. A retrieval model is used to find relevant information from existing sources in a RAG-based AI system. In contrast, the generative model takes the retrieved information, synthesizes all the data, and shapes it into a coherent and contextually appropriate response. What are the benefits of retrieval augmented generation? By integrating retrieval and generative artificial intelligence (AI) models, RAG delivers more accurate, relevant, and original responses while sounding like they came from humans. That’s because RAG models can understand the context of queries and generate fresh and unique replies by combining the best of both models. More accurate — Using a retrieval model to identify relevant information from existing knowledge sources, the original human-like responses that are subsequently generated are based on more relevant and up-to-date information than a pure generative model. Better at synthesizing information — By combining retrieval and generative models, RAG can synthesize information from numerous sources and generate fresh responses in a human-like way. This is particularly helpful for more complex queries that require integrating information from multiple sources. Adept at putting information into context — Unlike simple retrieval models, RAG can generate responses that are aware of the context of a conversation and are thus more relevant. Easier to train — Training an NLP-based large language model (LLM) to build a generative AI model requires a tremendous volume of data. Alternatively, RAG models use preexisting and pre-retrieved knowledge sources, reducing the need to find and ingest massive amounts of training data. More efficient — RAG models can be more efficient than large-scale generative models, as the initial retrieval phase narrows down the context and thus the volume of data that needs to be processed in the generation phase. How is retrieval augmented generation being used today? These are some real-life examples of how RAG models are being used today to: Improve customer support — RAG can be used to build advanced chatbots or virtual assistants that deliver more personalized and accurate responses to customer queries. This can lead to faster responses, increased operational efficiencies, and, eventually, greater customer satisfaction with support experiences. Generate content — RAG can help businesses produce blog posts, articles, product catalogs, or other content by combining its generative capabilities with retrieving information from reliable sources, both external and internal. Perform market research — By gathering insights from the vast volumes of data available on the internet—such as breaking news, industry research reports, even social media posts—RAG can keep businesses updated on market trends and even analyze competitors’ activities, helping companies to make better decisions. Support sales — RAG can serve as a virtual sales assistant, answering customers’ questions about items in inventory, retrieving product specifications, explaining operating instructions, and in general, assisting in the purchasing lifecycle. By marrying its generative abilities with product catalogs, pricing information, and other data—even customer reviews on social media—RAG can offer personalized recommendations, address customers’ concerns, and improve shopping experiences. Improve employee experience — RAG can help employees create and share a centralized repository of expert knowledge. By integrating with internal databases and documents, RAG can give employees accurate answers to questions about company operations, benefits, processes, culture, organizational structure, and more. Cohesity and AI Cohesity is at the forefront of the dawning age of AI because the Cohesity platform is ‘AI-ready’ for RAG-based large language models (LLM). The ground-breaking Cohesity approach provides robust and domain-specific context to RAG-driven AI systems by leveraging the robust file system of the Cohesity patented SnapTree and SpanFS architectures. To achieve this, an on-demand index of embeddings will be provided just-in-time to the AI application requesting the data. Additionally, the data will be secured through Cohesity’s role-based access control (RBAC) models. Cohesity Gaia utilizes RAG AI to search and summarize content using everyday language to create conversational queries. The Cohesity Gaia RAG platform accepts human and machine-driven input, such as questions and queries. That input is then tokenized with keywords that quickly filter petabytes of enterprise backup data down to a smaller subset of contextualized data. It then selects representations within those documents or objects most relevant to the question or query. That result is packaged, along with the original query, to an LLM such as GPT4 to provide a context-aware and human-sounding answer. This innovative approach ensures that the generated responses are knowledgeable, up-to-date, diverse, and relevant to the specific business content. By layering RAG on top of an enterprise’s datasets, Cohesity customers will not need to perform costly fine-tuning or extended training on vast volumes of data to teach LLMs “what to say.” This saves time and money and reduces environmental impact since RAG models are flexible enough to adapt to rapidly growing and constantly changing datasets. For this reason, leveraging RAG on the Cohesity platform can provide the most recent and relevant context to any query. Cohesity’s RAG-aware platform will generate more knowledgeable, diverse, and relevant responses compared to off-the-shelf LLMs without massively increasing data storage requirements. This breakthrough has tremendous potential for innovations with enterprise Q&A (questions and answers) applications and industry search and discovery models. Technology and business executives alike will have a unique opportunity to leverage the power of data-driven insights to enhance the quality of AI-driven conversations with Cohesity’s RAG-driven AI system. Organizations can unleash new levels of efficiency, innovation, and growth by harnessing the power of Cohesity data management and security solutions enhanced by AI. To learn more, read the AI eBook. Related Glossary Terms Artificial Intelligence (AI) Cognitive AI Generative AI You may also like Get started today You are now leaving the German section of www.cohesity.com/de/ and come to an English section of the site. Please click if you want to continue. Don't show this warning again You are now leaving the German section of www.cohesity.com/de/ and come to an English section of the site. Please click if you want to continue. Don't show this warning again\",\n",
              " 'What is Cloud Computing? Cloud Computing Concepts Hub Artificial Intelligence Generative AI What is RAG (Retrieval-Augmented Generation)? What is Retrieval-Augmented Generation? Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization\\'s internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. Why is Retrieval-Augmented Generation important? LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has. Known challenges of LLMs include: Presenting false information when it does not have the answer. Presenting out-of-date or generic information when the user expects a specific, current response. Creating a response from non-authoritative sources. Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things. You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate! RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response. What are the benefits of Retrieval-Augmented Generation? RAG technology brings several benefits to an organization\\'s generative AI efforts. Cost-effective implementation Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable. Current information Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users. Enhanced user trust RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution. More developer control With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM\\'s information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications. How does Retrieval-Augmented Generation work? Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process. Create external data The new data outside of the LLM\\'s original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand. Retrieve relevant information The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee\\'s past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations. Augment the LLM prompt Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries. Update external data The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used. The following diagram shows the conceptual flow of using RAG with LLMs. What is the difference between Retrieval-Augmented Generation and semantic search? Semantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications. Modern enterprises store vast amounts of information like manuals, FAQs, research reports, customer service guides, and human resource document repositories across various systems. Context retrieval is challenging at scale and consequently lowers generative output quality. Semantic search technologies can scan large databases of disparate information and retrieve data more accurately. For example, they can answer questions such as, \"How much was spent on machinery repairs last year?” by mapping the question to the relevant documents and returning specific text instead of search results. Developers can then use that answer to provide more context to the LLM. Conventional or keyword search solutions in RAG produce limited results for knowledge-intensive tasks. Developers must also deal with word embeddings, document chunking, and other complexities as they manually prepare their data. In contrast, semantic search technologies do all the work of knowledge base preparation so developers don\\'t have to. They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload. How can AWS support your Retrieval-Augmented Generation requirements? Amazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models—along with a broad set of capabilities—to build generative AI applications while simplifying development and maintaining privacy and security. With knowledge bases for Amazon Bedrock, you can connect FMs to your data sources for RAG in just a few clicks. Vector conversions, retrievals, and improved output generation are all handled automatically. For organizations managing their own RAG, Amazon Kendra is a highly-accurate enterprise search service powered by machine learning. It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra’s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows. For example, with the Retrieve API, you can: Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance. Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites. Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files. Filter responses based on those documents that the end-user permissions allow. Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples. Get started with Retrieval-Augmented Generation on AWS by creating a free account today Next Steps on AWS Instant get access to the AWS Free Tier. Get started building in the AWS management console.',\n",
              " 'What is Retrieval-Augmented Generation (RAG)? RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs). By combining your data and world knowledge with LLM language skills, grounded generation is more accurate, up-to-date, and relevant to your specific needs. Check out this e-book to unlock your “Enterprise Truth.” How does Retrieval-Augmented Generation work? RAGs operate with a few main steps to help enhance generative AI outputs: Retrieval and pre-processing: RAGs leverage powerful search algorithms to query external data, such as web pages, knowledge bases, and databases. Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words. Grounded generation: The pre-processed retrieved information is then seamlessly incorporated into the pre-trained LLM. This integration enhances the LLM\\'s context, providing it with a more comprehensive understanding of the topic. This augmented context enables the LLM to generate more precise, informative, and engaging responses. Why Use RAG? RAG offers several advantages augmenting traditional methods of text generation, especially when dealing with factual information or data-driven responses. Here are some key reasons why using RAG can be beneficial: Access to fresh information LLMs are limited to their pre-trained data. This leads to outdated and potentially inaccurate responses. RAG overcomes this by providing up-to-date information to LLMs. Factual grounding LLMs are powerful tools for generating creative and engaging text, but they can sometimes struggle with factual accuracy. This is because LLMs are trained on massive amounts of text data, which may contain inaccuracies or biases. Providing “facts” to the LLM as part of the input prompt can mitigate “gen AI hallucinations.” The crux of this approach is ensuring that the most relevant facts are provided to the LLM, and that the LLM output is entirely grounded on those facts while also answering the user’s question and adhering to system instructions and safety constraints. Using Gemini’s long context window (LCW) is a great way to provide source materials to the LLM. If you need to provide more information than fits into the LCW, or if you need to scale up performance, you can use a RAG approach that will reduce the number of tokens, saving you time and cost. Search with vector databases and relevancy re-rankers RAGs usually retrieve facts via search, and modern search engines now leverage vector databases to efficiently retrieve relevant documents. Vector databases store documents as embeddings in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity. Multi-modal embeddings can be used for images, audio and video, and more and these media embeddings can be retrieved alongside text embeddings or multi-language embeddings. Advanced search engines like Vertex AI Search use semantic search and keyword search together (called hybrid search), and a re-ranker which scores search results to ensure the top returned results are the most relevant. Additionally searches perform better with a clear, focused query without misspellings; so prior to lookup, sophisticated search engines will transform a query and fix spelling mistakes. Relevance, accuracy, and quality The retrieval mechanism in RAG is critically important. You need the best semantic search on top of a curated knowledge base to ensure that the retrieved information is relevant to the input query or context. If your retrieved information is irrelevant, your generation could be grounded but off-topic or incorrect. By fine-tuning or prompt-engineering the LLM to generate text entirely based on the retrieved knowledge, RAG helps to minimize contradictions and inconsistencies in the generated text. This significantly improves the quality of the generated text, and improves the user experience. The Vertex Eval Service now scores LLM generated text and retrieved chunks on metrics like “coherence,” “fluency,” “groundedness,” \"safety,\" “instruction_following,” “question_answering_quality,” and more. These metrics help you measure the grounded text you get from the LLM (for some metrics that is a comparison to a ground truth answer you have provided). Implementing these evaluations gives you a baseline measurement and you can optimize for RAG quality by configuring your search engine, curating your source data, improving source layout parsing or chunking strategies, or refining the user’s question prior to search. A RAG Ops, metrics driven approach like this will help you hill climb to high quality RAG and grounded generation. RAGs, agents, and chatbots RAG and grounding can be integrated into any LLM application or agent which needs access to fresh, private, or specialized data. By accessing external information, RAG-powered chatbots and conversational agents leverage external knowledge to provide more comprehensive, informative, and context-aware responses, improving the overall user experience. Your data and your use case are what differentiate what you are building with gen AI. RAG and grounding bring your data to LLMs efficiently and scalably. What Google Cloud products and services are related to RAG? The following Google Cloud products are related to Retrieval-Augmented Generation: Vertex AI SearchVertex AI Search is Google Search for your data, a fully managed, out-of-the-box search and RAG builder. Vertex AI Vector SearchThe ultra performant vector index that powers Vertex AI Search; it enables semantic and hybrid search and retrieval from huge collections of embeddings with high recall at high query rate. BigQueryLarge datasets that you can use to train machine learning models, including models for Vertex AI Vector Search. Grounded Generation APIGemini high-fidelity mode grounded with Google Search or inline facts or bring your own search engine. AlloyDBRun models in Vertex AI and access them in your application using familiar SQL queries. Use Google models, such as Gemini, or your own custom models. LlamaIndex on VertexBuild your own search engine for RAG and grounding using Google or open source components and our fully managed orchestration system based on LlamaIndex. Learn more about using retrieval augmented generation with these resources. Using Vertex AI to build next-gen search applications | Google Cloud Blog RAGs powered by Google Search technology RAG with databases on Google Cloud Infrastructure for a RAG-capable generative AI application using Vertex AI APIs to build your own search and Retrieval Augmented Generation (RAG) systems How to use RAG in BigQuery to bolster LLMs Code sample and quickstart to get familiar with RAG Start building on Google Cloud with $300 in free credits and 20+ always free products. Need help getting started?Contact sales Work with a trusted partnerFind a partner Continue browsingSee all products']"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ruAwMmPbN2_",
        "outputId": "8b2de721-6a80-4ebf-b6f1-605aa1c8adab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts[0]), len(texts[1]), len(texts[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLAKCScYYYOs",
        "outputId": "f47183af-4ab9-44c5-b5ca-3d8509b8000a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8422, 10109, 7083)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P1rpe-fHH5H"
      },
      "source": [
        "# Cleaning/Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z09FAPueHH5I"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "texts[0] = clean_text(texts[0])\n",
        "texts[1] = clean_text(texts[1])\n",
        "texts[2] = clean_text(texts[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc4jKQu2bLZi",
        "outputId": "7eff584d-68a8-4cb6-df1f-d804391398bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Cohesity named a Leader again! 2023 Gartner® Magic Quadrant™ for Enterprise Backup and Recovery Software Solutions Glossary Retrieval augmented generation (RAG) Retrieval augmented generation (RAG) What is retrieval augmented generation (RAG)? Retrieval augmented generation (RAG) is a natural language processing (NLP) technique that combines the strengths of both retrieval- and generative-based artificial intelligence (AI) models. RAG AI can deliver accurate results that make the most of pre-existing knowledge but can also process and consolidate that knowledge to create unique, context-aware answers, instructions, or explanations in human-like language rather than just summarizing the retrieved data. RAG AI differs from generative AI in that it is a superset of generative AI. RAG combines the strengths of both generative AI and retrieval AI. RAG is also different from cognitive AI, which mimics the way the human brain works to get its results. How does retrieval augmented generation (RAG) work? RAG, short for retrieval augmented generation, works by integrating retrieval-based techniques with generative-based AI models. Retrieval-based models excel at extracting information from pre-existing online sources like newspaper articles, databases, blogs, and other knowledge repositories such as Wikipedia or even internal databases. However, such models cannot produce original or unique responses. Alternatively, generative models can generate original responses that are appropriate within the context of what is being asked, but can find it difficult to maintain strict accuracy. RAG was developed to overcome these relative weaknesses in existing models to combine their strengths and minimize drawbacks. A retrieval model is used to find relevant information from existing sources in a RAG-based AI system. In contrast, the generative model takes the retrieved information, synthesizes all the data, and shapes it into a coherent and contextually appropriate response. What are the benefits of retrieval augmented generation? By integrating retrieval and generative artificial intelligence (AI) models, RAG delivers more accurate, relevant, and original responses while sounding like they came from humans. That’s because RAG models can understand the context of queries and generate fresh and unique replies by combining the best of both models. More accurate — Using a retrieval model to identify relevant information from existing knowledge sources, the original human-like responses that are subsequently generated are based on more relevant and up-to-date information than a pure generative model. Better at synthesizing information — By combining retrieval and generative models, RAG can synthesize information from numerous sources and generate fresh responses in a human-like way. This is particularly helpful for more complex queries that require integrating information from multiple sources. Adept at putting information into context — Unlike simple retrieval models, RAG can generate responses that are aware of the context of a conversation and are thus more relevant. Easier to train — Training an NLP-based large language model (LLM) to build a generative AI model requires a tremendous volume of data. Alternatively, RAG models use preexisting and pre-retrieved knowledge sources, reducing the need to find and ingest massive amounts of training data. More efficient — RAG models can be more efficient than large-scale generative models, as the initial retrieval phase narrows down the context and thus the volume of data that needs to be processed in the generation phase. How is retrieval augmented generation being used today? These are some real-life examples of how RAG models are being used today to: Improve customer support — RAG can be used to build advanced chatbots or virtual assistants that deliver more personalized and accurate responses to customer queries. This can lead to faster responses, increased operational efficiencies, and, eventually, greater customer satisfaction with support experiences. Generate content — RAG can help businesses produce blog posts, articles, product catalogs, or other content by combining its generative capabilities with retrieving information from reliable sources, both external and internal. Perform market research — By gathering insights from the vast volumes of data available on the internet—such as breaking news, industry research reports, even social media posts—RAG can keep businesses updated on market trends and even analyze competitors’ activities, helping companies to make better decisions. Support sales — RAG can serve as a virtual sales assistant, answering customers’ questions about items in inventory, retrieving product specifications, explaining operating instructions, and in general, assisting in the purchasing lifecycle. By marrying its generative abilities with product catalogs, pricing information, and other data—even customer reviews on social media—RAG can offer personalized recommendations, address customers’ concerns, and improve shopping experiences. Improve employee experience — RAG can help employees create and share a centralized repository of expert knowledge. By integrating with internal databases and documents, RAG can give employees accurate answers to questions about company operations, benefits, processes, culture, organizational structure, and more. Cohesity and AI Cohesity is at the forefront of the dawning age of AI because the Cohesity platform is ‘AI-ready’ for RAG-based large language models (LLM). The ground-breaking Cohesity approach provides robust and domain-specific context to RAG-driven AI systems by leveraging the robust file system of the Cohesity patented SnapTree and SpanFS architectures. To achieve this, an on-demand index of embeddings will be provided just-in-time to the AI application requesting the data. Additionally, the data will be secured through Cohesity’s role-based access control (RBAC) models. Cohesity Gaia utilizes RAG AI to search and summarize content using everyday language to create conversational queries. The Cohesity Gaia RAG platform accepts human and machine-driven input, such as questions and queries. That input is then tokenized with keywords that quickly filter petabytes of enterprise backup data down to a smaller subset of contextualized data. It then selects representations within those documents or objects most relevant to the question or query. That result is packaged, along with the original query, to an LLM such as GPT4 to provide a context-aware and human-sounding answer. This innovative approach ensures that the generated responses are knowledgeable, up-to-date, diverse, and relevant to the specific business content. By layering RAG on top of an enterprise’s datasets, Cohesity customers will not need to perform costly fine-tuning or extended training on vast volumes of data to teach LLMs “what to say.” This saves time and money and reduces environmental impact since RAG models are flexible enough to adapt to rapidly growing and constantly changing datasets. For this reason, leveraging RAG on the Cohesity platform can provide the most recent and relevant context to any query. Cohesity’s RAG-aware platform will generate more knowledgeable, diverse, and relevant responses compared to off-the-shelf LLMs without massively increasing data storage requirements. This breakthrough has tremendous potential for innovations with enterprise Q&A (questions and answers) applications and industry search and discovery models. Technology and business executives alike will have a unique opportunity to leverage the power of data-driven insights to enhance the quality of AI-driven conversations with Cohesity’s RAG-driven AI system. Organizations can unleash new levels of efficiency, innovation, and growth by harnessing the power of Cohesity data management and security solutions enhanced by AI. To learn more, read the AI eBook. Related Glossary Terms Artificial Intelligence (AI) Cognitive AI Generative AI You may also like Get started today You are now leaving the German section of www.cohesity.com/de/ and come to an English section of the site. Please click if you want to continue. Don't show this warning again You are now leaving the German section of www.cohesity.com/de/ and come to an English section of the site. Please click if you want to continue. Don't show this warning again\",\n",
              " 'What is Cloud Computing? Cloud Computing Concepts Hub Artificial Intelligence Generative AI What is RAG (Retrieval-Augmented Generation)? What is Retrieval-Augmented Generation? Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization\\'s internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. Why is Retrieval-Augmented Generation important? LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has. Known challenges of LLMs include: Presenting false information when it does not have the answer. Presenting out-of-date or generic information when the user expects a specific, current response. Creating a response from non-authoritative sources. Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things. You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate! RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response. What are the benefits of Retrieval-Augmented Generation? RAG technology brings several benefits to an organization\\'s generative AI efforts. Cost-effective implementation Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable. Current information Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users. Enhanced user trust RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution. More developer control With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM\\'s information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications. How does Retrieval-Augmented Generation work? Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process. Create external data The new data outside of the LLM\\'s original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand. Retrieve relevant information The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee\\'s past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations. Augment the LLM prompt Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries. Update external data The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used. The following diagram shows the conceptual flow of using RAG with LLMs. What is the difference between Retrieval-Augmented Generation and semantic search? Semantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications. Modern enterprises store vast amounts of information like manuals, FAQs, research reports, customer service guides, and human resource document repositories across various systems. Context retrieval is challenging at scale and consequently lowers generative output quality. Semantic search technologies can scan large databases of disparate information and retrieve data more accurately. For example, they can answer questions such as, \"How much was spent on machinery repairs last year?” by mapping the question to the relevant documents and returning specific text instead of search results. Developers can then use that answer to provide more context to the LLM. Conventional or keyword search solutions in RAG produce limited results for knowledge-intensive tasks. Developers must also deal with word embeddings, document chunking, and other complexities as they manually prepare their data. In contrast, semantic search technologies do all the work of knowledge base preparation so developers don\\'t have to. They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload. How can AWS support your Retrieval-Augmented Generation requirements? Amazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models—along with a broad set of capabilities—to build generative AI applications while simplifying development and maintaining privacy and security. With knowledge bases for Amazon Bedrock, you can connect FMs to your data sources for RAG in just a few clicks. Vector conversions, retrievals, and improved output generation are all handled automatically. For organizations managing their own RAG, Amazon Kendra is a highly-accurate enterprise search service powered by machine learning. It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra’s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows. For example, with the Retrieve API, you can: Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance. Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites. Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files. Filter responses based on those documents that the end-user permissions allow. Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples. Get started with Retrieval-Augmented Generation on AWS by creating a free account today Next Steps on AWS Instant get access to the AWS Free Tier. Get started building in the AWS management console.',\n",
              " 'What is Retrieval-Augmented Generation (RAG)? RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs). By combining your data and world knowledge with LLM language skills, grounded generation is more accurate, up-to-date, and relevant to your specific needs. Check out this e-book to unlock your “Enterprise Truth.” How does Retrieval-Augmented Generation work? RAGs operate with a few main steps to help enhance generative AI outputs: Retrieval and pre-processing: RAGs leverage powerful search algorithms to query external data, such as web pages, knowledge bases, and databases. Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words. Grounded generation: The pre-processed retrieved information is then seamlessly incorporated into the pre-trained LLM. This integration enhances the LLM\\'s context, providing it with a more comprehensive understanding of the topic. This augmented context enables the LLM to generate more precise, informative, and engaging responses. Why Use RAG? RAG offers several advantages augmenting traditional methods of text generation, especially when dealing with factual information or data-driven responses. Here are some key reasons why using RAG can be beneficial: Access to fresh information LLMs are limited to their pre-trained data. This leads to outdated and potentially inaccurate responses. RAG overcomes this by providing up-to-date information to LLMs. Factual grounding LLMs are powerful tools for generating creative and engaging text, but they can sometimes struggle with factual accuracy. This is because LLMs are trained on massive amounts of text data, which may contain inaccuracies or biases. Providing “facts” to the LLM as part of the input prompt can mitigate “gen AI hallucinations.” The crux of this approach is ensuring that the most relevant facts are provided to the LLM, and that the LLM output is entirely grounded on those facts while also answering the user’s question and adhering to system instructions and safety constraints. Using Gemini’s long context window (LCW) is a great way to provide source materials to the LLM. If you need to provide more information than fits into the LCW, or if you need to scale up performance, you can use a RAG approach that will reduce the number of tokens, saving you time and cost. Search with vector databases and relevancy re-rankers RAGs usually retrieve facts via search, and modern search engines now leverage vector databases to efficiently retrieve relevant documents. Vector databases store documents as embeddings in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity. Multi-modal embeddings can be used for images, audio and video, and more and these media embeddings can be retrieved alongside text embeddings or multi-language embeddings. Advanced search engines like Vertex AI Search use semantic search and keyword search together (called hybrid search), and a re-ranker which scores search results to ensure the top returned results are the most relevant. Additionally searches perform better with a clear, focused query without misspellings; so prior to lookup, sophisticated search engines will transform a query and fix spelling mistakes. Relevance, accuracy, and quality The retrieval mechanism in RAG is critically important. You need the best semantic search on top of a curated knowledge base to ensure that the retrieved information is relevant to the input query or context. If your retrieved information is irrelevant, your generation could be grounded but off-topic or incorrect. By fine-tuning or prompt-engineering the LLM to generate text entirely based on the retrieved knowledge, RAG helps to minimize contradictions and inconsistencies in the generated text. This significantly improves the quality of the generated text, and improves the user experience. The Vertex Eval Service now scores LLM generated text and retrieved chunks on metrics like “coherence,” “fluency,” “groundedness,” \"safety,\" “instruction_following,” “question_answering_quality,” and more. These metrics help you measure the grounded text you get from the LLM (for some metrics that is a comparison to a ground truth answer you have provided). Implementing these evaluations gives you a baseline measurement and you can optimize for RAG quality by configuring your search engine, curating your source data, improving source layout parsing or chunking strategies, or refining the user’s question prior to search. A RAG Ops, metrics driven approach like this will help you hill climb to high quality RAG and grounded generation. RAGs, agents, and chatbots RAG and grounding can be integrated into any LLM application or agent which needs access to fresh, private, or specialized data. By accessing external information, RAG-powered chatbots and conversational agents leverage external knowledge to provide more comprehensive, informative, and context-aware responses, improving the overall user experience. Your data and your use case are what differentiate what you are building with gen AI. RAG and grounding bring your data to LLMs efficiently and scalably. What Google Cloud products and services are related to RAG? The following Google Cloud products are related to Retrieval-Augmented Generation: Vertex AI SearchVertex AI Search is Google Search for your data, a fully managed, out-of-the-box search and RAG builder. Vertex AI Vector SearchThe ultra performant vector index that powers Vertex AI Search; it enables semantic and hybrid search and retrieval from huge collections of embeddings with high recall at high query rate. BigQueryLarge datasets that you can use to train machine learning models, including models for Vertex AI Vector Search. Grounded Generation APIGemini high-fidelity mode grounded with Google Search or inline facts or bring your own search engine. AlloyDBRun models in Vertex AI and access them in your application using familiar SQL queries. Use Google models, such as Gemini, or your own custom models. LlamaIndex on VertexBuild your own search engine for RAG and grounding using Google or open source components and our fully managed orchestration system based on LlamaIndex. Learn more about using retrieval augmented generation with these resources. Using Vertex AI to build next-gen search applications | Google Cloud Blog RAGs powered by Google Search technology RAG with databases on Google Cloud Infrastructure for a RAG-capable generative AI application using Vertex AI APIs to build your own search and Retrieval Augmented Generation (RAG) systems How to use RAG in BigQuery to bolster LLMs Code sample and quickstart to get familiar with RAG Start building on Google Cloud with $300 in free credits and 20+ always free products. Need help getting started?Contact sales Work with a trusted partnerFind a partner Continue browsingSee all products']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLXJQdcdHH5I",
        "outputId": "111b3f1a-4c2d-46f0-a64b-761d06a359f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8413, 10105, 7081)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "len(texts[0]), len(texts[1]), len(texts[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_text = \" \".join(texts)\n",
        "final_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "mku22u_ZbuF-",
        "outputId": "53dafb3a-d67c-4297-f0a8-7969c1fcb3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cohesity named a Leader again! 2023 Gartner® Magic Quadrant™ for Enterprise Backup and Recovery Software Solutions Glossary Retrieval augmented generation (RAG) Retrieval augmented generation (RAG) What is retrieval augmented generation (RAG)? Retrieval augmented generation (RAG) is a natural language processing (NLP) technique that combines the strengths of both retrieval- and generative-based artificial intelligence (AI) models. RAG AI can deliver accurate results that make the most of pre-existing knowledge but can also process and consolidate that knowledge to create unique, context-aware answers, instructions, or explanations in human-like language rather than just summarizing the retrieved data. RAG AI differs from generative AI in that it is a superset of generative AI. RAG combines the strengths of both generative AI and retrieval AI. RAG is also different from cognitive AI, which mimics the way the human brain works to get its results. How does retrieval augmented generation (RAG) work? RAG, short for retrieval augmented generation, works by integrating retrieval-based techniques with generative-based AI models. Retrieval-based models excel at extracting information from pre-existing online sources like newspaper articles, databases, blogs, and other knowledge repositories such as Wikipedia or even internal databases. However, such models cannot produce original or unique responses. Alternatively, generative models can generate original responses that are appropriate within the context of what is being asked, but can find it difficult to maintain strict accuracy. RAG was developed to overcome these relative weaknesses in existing models to combine their strengths and minimize drawbacks. A retrieval model is used to find relevant information from existing sources in a RAG-based AI system. In contrast, the generative model takes the retrieved information, synthesizes all the data, and shapes it into a coherent and contextually appropriate response. What are the benefits of retrieval augmented generation? By integrating retrieval and generative artificial intelligence (AI) models, RAG delivers more accurate, relevant, and original responses while sounding like they came from humans. That’s because RAG models can understand the context of queries and generate fresh and unique replies by combining the best of both models. More accurate — Using a retrieval model to identify relevant information from existing knowledge sources, the original human-like responses that are subsequently generated are based on more relevant and up-to-date information than a pure generative model. Better at synthesizing information — By combining retrieval and generative models, RAG can synthesize information from numerous sources and generate fresh responses in a human-like way. This is particularly helpful for more complex queries that require integrating information from multiple sources. Adept at putting information into context — Unlike simple retrieval models, RAG can generate responses that are aware of the context of a conversation and are thus more relevant. Easier to train — Training an NLP-based large language model (LLM) to build a generative AI model requires a tremendous volume of data. Alternatively, RAG models use preexisting and pre-retrieved knowledge sources, reducing the need to find and ingest massive amounts of training data. More efficient — RAG models can be more efficient than large-scale generative models, as the initial retrieval phase narrows down the context and thus the volume of data that needs to be processed in the generation phase. How is retrieval augmented generation being used today? These are some real-life examples of how RAG models are being used today to: Improve customer support — RAG can be used to build advanced chatbots or virtual assistants that deliver more personalized and accurate responses to customer queries. This can lead to faster responses, increased operational efficiencies, and, eventually, greater customer satisfaction with support experiences. Generate content — RAG can help businesses produce blog posts, articles, product catalogs, or other content by combining its generative capabilities with retrieving information from reliable sources, both external and internal. Perform market research — By gathering insights from the vast volumes of data available on the internet—such as breaking news, industry research reports, even social media posts—RAG can keep businesses updated on market trends and even analyze competitors’ activities, helping companies to make better decisions. Support sales — RAG can serve as a virtual sales assistant, answering customers’ questions about items in inventory, retrieving product specifications, explaining operating instructions, and in general, assisting in the purchasing lifecycle. By marrying its generative abilities with product catalogs, pricing information, and other data—even customer reviews on social media—RAG can offer personalized recommendations, address customers’ concerns, and improve shopping experiences. Improve employee experience — RAG can help employees create and share a centralized repository of expert knowledge. By integrating with internal databases and documents, RAG can give employees accurate answers to questions about company operations, benefits, processes, culture, organizational structure, and more. Cohesity and AI Cohesity is at the forefront of the dawning age of AI because the Cohesity platform is ‘AI-ready’ for RAG-based large language models (LLM). The ground-breaking Cohesity approach provides robust and domain-specific context to RAG-driven AI systems by leveraging the robust file system of the Cohesity patented SnapTree and SpanFS architectures. To achieve this, an on-demand index of embeddings will be provided just-in-time to the AI application requesting the data. Additionally, the data will be secured through Cohesity’s role-based access control (RBAC) models. Cohesity Gaia utilizes RAG AI to search and summarize content using everyday language to create conversational queries. The Cohesity Gaia RAG platform accepts human and machine-driven input, such as questions and queries. That input is then tokenized with keywords that quickly filter petabytes of enterprise backup data down to a smaller subset of contextualized data. It then selects representations within those documents or objects most relevant to the question or query. That result is packaged, along with the original query, to an LLM such as GPT4 to provide a context-aware and human-sounding answer. This innovative approach ensures that the generated responses are knowledgeable, up-to-date, diverse, and relevant to the specific business content. By layering RAG on top of an enterprise’s datasets, Cohesity customers will not need to perform costly fine-tuning or extended training on vast volumes of data to teach LLMs “what to say.” This saves time and money and reduces environmental impact since RAG models are flexible enough to adapt to rapidly growing and constantly changing datasets. For this reason, leveraging RAG on the Cohesity platform can provide the most recent and relevant context to any query. Cohesity’s RAG-aware platform will generate more knowledgeable, diverse, and relevant responses compared to off-the-shelf LLMs without massively increasing data storage requirements. This breakthrough has tremendous potential for innovations with enterprise Q&A (questions and answers) applications and industry search and discovery models. Technology and business executives alike will have a unique opportunity to leverage the power of data-driven insights to enhance the quality of AI-driven conversations with Cohesity’s RAG-driven AI system. Organizations can unleash new levels of efficiency, innovation, and growth by harnessing the power of Cohesity data management and security solutions enhanced by AI. To learn more, read the AI eBook. Related Glossary Terms Artificial Intelligence (AI) Cognitive AI Generative AI You may also like Get started today You are now leaving the German section of www.cohesity.com/de/ and come to an English section of the site. Please click if you want to continue. Don\\'t show this warning again You are now leaving the German section of www.cohesity.com/de/ and come to an English section of the site. Please click if you want to continue. Don\\'t show this warning again What is Cloud Computing? Cloud Computing Concepts Hub Artificial Intelligence Generative AI What is RAG (Retrieval-Augmented Generation)? What is Retrieval-Augmented Generation? Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization\\'s internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. Why is Retrieval-Augmented Generation important? LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has. Known challenges of LLMs include: Presenting false information when it does not have the answer. Presenting out-of-date or generic information when the user expects a specific, current response. Creating a response from non-authoritative sources. Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things. You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate! RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response. What are the benefits of Retrieval-Augmented Generation? RAG technology brings several benefits to an organization\\'s generative AI efforts. Cost-effective implementation Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable. Current information Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users. Enhanced user trust RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution. More developer control With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM\\'s information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications. How does Retrieval-Augmented Generation work? Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process. Create external data The new data outside of the LLM\\'s original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand. Retrieve relevant information The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee\\'s past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations. Augment the LLM prompt Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries. Update external data The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used. The following diagram shows the conceptual flow of using RAG with LLMs. What is the difference between Retrieval-Augmented Generation and semantic search? Semantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications. Modern enterprises store vast amounts of information like manuals, FAQs, research reports, customer service guides, and human resource document repositories across various systems. Context retrieval is challenging at scale and consequently lowers generative output quality. Semantic search technologies can scan large databases of disparate information and retrieve data more accurately. For example, they can answer questions such as, \"How much was spent on machinery repairs last year?” by mapping the question to the relevant documents and returning specific text instead of search results. Developers can then use that answer to provide more context to the LLM. Conventional or keyword search solutions in RAG produce limited results for knowledge-intensive tasks. Developers must also deal with word embeddings, document chunking, and other complexities as they manually prepare their data. In contrast, semantic search technologies do all the work of knowledge base preparation so developers don\\'t have to. They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload. How can AWS support your Retrieval-Augmented Generation requirements? Amazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models—along with a broad set of capabilities—to build generative AI applications while simplifying development and maintaining privacy and security. With knowledge bases for Amazon Bedrock, you can connect FMs to your data sources for RAG in just a few clicks. Vector conversions, retrievals, and improved output generation are all handled automatically. For organizations managing their own RAG, Amazon Kendra is a highly-accurate enterprise search service powered by machine learning. It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra’s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows. For example, with the Retrieve API, you can: Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance. Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites. Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files. Filter responses based on those documents that the end-user permissions allow. Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples. Get started with Retrieval-Augmented Generation on AWS by creating a free account today Next Steps on AWS Instant get access to the AWS Free Tier. Get started building in the AWS management console. What is Retrieval-Augmented Generation (RAG)? RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs). By combining your data and world knowledge with LLM language skills, grounded generation is more accurate, up-to-date, and relevant to your specific needs. Check out this e-book to unlock your “Enterprise Truth.” How does Retrieval-Augmented Generation work? RAGs operate with a few main steps to help enhance generative AI outputs: Retrieval and pre-processing: RAGs leverage powerful search algorithms to query external data, such as web pages, knowledge bases, and databases. Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words. Grounded generation: The pre-processed retrieved information is then seamlessly incorporated into the pre-trained LLM. This integration enhances the LLM\\'s context, providing it with a more comprehensive understanding of the topic. This augmented context enables the LLM to generate more precise, informative, and engaging responses. Why Use RAG? RAG offers several advantages augmenting traditional methods of text generation, especially when dealing with factual information or data-driven responses. Here are some key reasons why using RAG can be beneficial: Access to fresh information LLMs are limited to their pre-trained data. This leads to outdated and potentially inaccurate responses. RAG overcomes this by providing up-to-date information to LLMs. Factual grounding LLMs are powerful tools for generating creative and engaging text, but they can sometimes struggle with factual accuracy. This is because LLMs are trained on massive amounts of text data, which may contain inaccuracies or biases. Providing “facts” to the LLM as part of the input prompt can mitigate “gen AI hallucinations.” The crux of this approach is ensuring that the most relevant facts are provided to the LLM, and that the LLM output is entirely grounded on those facts while also answering the user’s question and adhering to system instructions and safety constraints. Using Gemini’s long context window (LCW) is a great way to provide source materials to the LLM. If you need to provide more information than fits into the LCW, or if you need to scale up performance, you can use a RAG approach that will reduce the number of tokens, saving you time and cost. Search with vector databases and relevancy re-rankers RAGs usually retrieve facts via search, and modern search engines now leverage vector databases to efficiently retrieve relevant documents. Vector databases store documents as embeddings in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity. Multi-modal embeddings can be used for images, audio and video, and more and these media embeddings can be retrieved alongside text embeddings or multi-language embeddings. Advanced search engines like Vertex AI Search use semantic search and keyword search together (called hybrid search), and a re-ranker which scores search results to ensure the top returned results are the most relevant. Additionally searches perform better with a clear, focused query without misspellings; so prior to lookup, sophisticated search engines will transform a query and fix spelling mistakes. Relevance, accuracy, and quality The retrieval mechanism in RAG is critically important. You need the best semantic search on top of a curated knowledge base to ensure that the retrieved information is relevant to the input query or context. If your retrieved information is irrelevant, your generation could be grounded but off-topic or incorrect. By fine-tuning or prompt-engineering the LLM to generate text entirely based on the retrieved knowledge, RAG helps to minimize contradictions and inconsistencies in the generated text. This significantly improves the quality of the generated text, and improves the user experience. The Vertex Eval Service now scores LLM generated text and retrieved chunks on metrics like “coherence,” “fluency,” “groundedness,” \"safety,\" “instruction_following,” “question_answering_quality,” and more. These metrics help you measure the grounded text you get from the LLM (for some metrics that is a comparison to a ground truth answer you have provided). Implementing these evaluations gives you a baseline measurement and you can optimize for RAG quality by configuring your search engine, curating your source data, improving source layout parsing or chunking strategies, or refining the user’s question prior to search. A RAG Ops, metrics driven approach like this will help you hill climb to high quality RAG and grounded generation. RAGs, agents, and chatbots RAG and grounding can be integrated into any LLM application or agent which needs access to fresh, private, or specialized data. By accessing external information, RAG-powered chatbots and conversational agents leverage external knowledge to provide more comprehensive, informative, and context-aware responses, improving the overall user experience. Your data and your use case are what differentiate what you are building with gen AI. RAG and grounding bring your data to LLMs efficiently and scalably. What Google Cloud products and services are related to RAG? The following Google Cloud products are related to Retrieval-Augmented Generation: Vertex AI SearchVertex AI Search is Google Search for your data, a fully managed, out-of-the-box search and RAG builder. Vertex AI Vector SearchThe ultra performant vector index that powers Vertex AI Search; it enables semantic and hybrid search and retrieval from huge collections of embeddings with high recall at high query rate. BigQueryLarge datasets that you can use to train machine learning models, including models for Vertex AI Vector Search. Grounded Generation APIGemini high-fidelity mode grounded with Google Search or inline facts or bring your own search engine. AlloyDBRun models in Vertex AI and access them in your application using familiar SQL queries. Use Google models, such as Gemini, or your own custom models. LlamaIndex on VertexBuild your own search engine for RAG and grounding using Google or open source components and our fully managed orchestration system based on LlamaIndex. Learn more about using retrieval augmented generation with these resources. Using Vertex AI to build next-gen search applications | Google Cloud Blog RAGs powered by Google Search technology RAG with databases on Google Cloud Infrastructure for a RAG-capable generative AI application using Vertex AI APIs to build your own search and Retrieval Augmented Generation (RAG) systems How to use RAG in BigQuery to bolster LLMs Code sample and quickstart to get familiar with RAG Start building on Google Cloud with $300 in free credits and 20+ always free products. Need help getting started?Contact sales Work with a trusted partnerFind a partner Continue browsingSee all products'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tVooRvGc-0q",
        "outputId": "38c35c6a-50f6-46d5-835c-545de8faa139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25601"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zj0rl_81wOn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWZsofpzHH5K"
      },
      "source": [
        "# Knowledge base using sentence embenddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss # Facebook AI Similarity Seacrh\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XyPToldRbWpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "s7lCVbO6bZrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = [final_text[i:i+1500] for i in range(0, len(final_text), 1500)] # chunking\n",
        "embeddings = model.encode(chunks) # embeddings for each chunk\n",
        "# embeddings array of the form (N,D) where N = no. of chunks and D = dim of embeddings vec"
      ],
      "metadata": {
        "id": "fyewXl81beX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7jU9Q-uHH5K",
        "outputId": "560f2a0c-11f9-43d2-9dca-68ed9cd3d021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexed 18 chunks.\n"
          ]
        }
      ],
      "source": [
        "dimension = embeddings.shape[1] # extract D\n",
        "index = faiss.IndexFlatL2(dimension) # initialize L2 eucidean distance for similarity b/w vecs; saying it has dimension number of dimensions for the vectors\n",
        "index.add(np.array(embeddings)) # add embeddings to FAISS index; FAISS build internal struct for optimal search\n",
        "print(f\"Indexed {len(chunks)} chunks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sToXoJscHH5M"
      },
      "source": [
        "# RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV0P1QUqHH5N"
      },
      "source": [
        "## Retrieval (3 nearest clusters using similarity search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWsGXbBbHH5N"
      },
      "outputs": [],
      "source": [
        "user_query = query\n",
        "\n",
        "query_embedding = model.encode([user_query]) # embeddings for query\n",
        "D, I = index.search(query_embedding, k=3)  # top 10 closest\n",
        "\n",
        "relevant_passages = [chunks[i] for i in I[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXD7oKSXHH5N"
      },
      "outputs": [],
      "source": [
        "relevant_passages_str = ''.join(relevant_passages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "uT4y60wcHH5O",
        "outputId": "19b75197-87b2-4896-df2d-8bdabbbc6438"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" pre-trained LLM. This integration enhances the LLM's context, providing it with a more comprehensive understanding of the topic. This augmented context enables the LLM to generate more precise, informative, and engaging responses. Why Use RAG? RAG offers several advantages augmenting traditional methods of text generation, especially when dealing with factual information or data-driven responses. Here are some key reasons why using RAG can be beneficial: Access to fresh information LLMs are limited to their pre-trained data. This leads to outdated and potentially inaccurate responses. RAG overcomes this by providing up-to-date information to LLMs. Factual grounding LLMs are powerful tools for generating creative and engaging text, but they can sometimes struggle with factual accuracy. This is because LLMs are trained on massive amounts of text data, which may contain inaccuracies or biases. Providing “facts” to the LLM as part of the input prompt can mitigate “gen AI hallucinations.” The crux of this approach is ensuring that the most relevant facts are provided to the LLM, and that the LLM output is entirely grounded on those facts while also answering the user’s question and adhering to system instructions and safety constraints. Using Gemini’s long context window (LCW) is a great way to provide source materials to the LLM. If you need to provide more information than fits into the LCW, or if you need to scale up performance, you can use a RAG approach that will reduce theCohesity named a Leader again! 2023 Gartner® Magic Quadrant™ for Enterprise Backup and Recovery Software Solutions Glossary Retrieval augmented generation (RAG) Retrieval augmented generation (RAG) What is retrieval augmented generation (RAG)? Retrieval augmented generation (RAG) is a natural language processing (NLP) technique that combines the strengths of both retrieval- and generative-based artificial intelligence (AI) models. RAG AI can deliver accurate results that make the most of pre-existing knowledge but can also process and consolidate that knowledge to create unique, context-aware answers, instructions, or explanations in human-like language rather than just summarizing the retrieved data. RAG AI differs from generative AI in that it is a superset of generative AI. RAG combines the strengths of both generative AI and retrieval AI. RAG is also different from cognitive AI, which mimics the way the human brain works to get its results. How does retrieval augmented generation (RAG) work? RAG, short for retrieval augmented generation, works by integrating retrieval-based techniques with generative-based AI models. Retrieval-based models excel at extracting information from pre-existing online sources like newspaper articles, databases, blogs, and other knowledge repositories such as Wikipedia or even internal databases. However, such models cannot produce original or unique responses. Alternatively, generative models can generate original responses that are appropriateet trends and even analyze competitors’ activities, helping companies to make better decisions. Support sales — RAG can serve as a virtual sales assistant, answering customers’ questions about items in inventory, retrieving product specifications, explaining operating instructions, and in general, assisting in the purchasing lifecycle. By marrying its generative abilities with product catalogs, pricing information, and other data—even customer reviews on social media—RAG can offer personalized recommendations, address customers’ concerns, and improve shopping experiences. Improve employee experience — RAG can help employees create and share a centralized repository of expert knowledge. By integrating with internal databases and documents, RAG can give employees accurate answers to questions about company operations, benefits, processes, culture, organizational structure, and more. Cohesity and AI Cohesity is at the forefront of the dawning age of AI because the Cohesity platform is ‘AI-ready’ for RAG-based large language models (LLM). The ground-breaking Cohesity approach provides robust and domain-specific context to RAG-driven AI systems by leveraging the robust file system of the Cohesity patented SnapTree and SpanFS architectures. To achieve this, an on-demand index of embeddings will be provided just-in-time to the AI application requesting the data. Additionally, the data will be secured through Cohesity’s role-based access control (RBAC) models. Cohesity Gaia utilizes \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "relevant_passages_str"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(relevant_passages_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRwcS9TfjVq_",
        "outputId": "7f57f4d8-9b39-484c-ddc9-8defaf87a48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4500"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzNkN_IZHH5O"
      },
      "source": [
        "## Augmented generation (with RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP4KGAXVHH5O"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": final_text},\n",
        "    {\"role\": \"user\", \"content\": user_query}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwAFtbbfHH5O",
        "outputId": "45812209-e8cf-4db0-e679-7c99bed7d8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A RAG system is a type of AI system that combines information retrieval and generative artificial intelligence (AI) models to generate human-like responses. RAG systems are designed to provide accurate, relevant, and original responses to user queries. They use pre-existing knowledge sources to provide a context-aware and human-sounding answer to the user's question. RAG systems are often used in fields such as customer support, marketing research, and business intelligence to improve the quality of AI-driven conversations with customers. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as finance, healthcare, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to user queries. RAG systems are often used in industries such as healthcare, finance, and manufacturing to provide personalized and accurate responses to\n",
            "\n",
            "Sources: \n",
            "\n",
            "https://www.cohesity.com/glossary/retrieval-augmented-generation-rag/\n",
            "https://aws.amazon.com/what-is/retrieval-augmented-generation/\n",
            "https://cloud.google.com/use-cases/retrieval-augmented-generation\n"
          ]
        }
      ],
      "source": [
        "print(response)\n",
        "print(\"\\nSources: \\n\")\n",
        "for url in final_urls:\n",
        "  print(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G1E6r3mMUpf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Control data (same prompt without RAG)"
      ],
      "metadata": {
        "id": "8KwDxVi_0jIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "KjPaweEsSQLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": user_query}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "amL5fgNv0P-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)\n"
      ],
      "metadata": {
        "id": "nzO0vMZY0TDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62517859-6770-44d3-f07f-0d7c2e54e1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A RAG system is a type of language model that can generate human-like text based on a given instruction. It uses natural language processing techniques to understand and respond to user queries and prompts in a conversational manner.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MC1qgr5D0pKZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}